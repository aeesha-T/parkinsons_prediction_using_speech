{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on MDVR-KCL Dataset using the .wav segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import auc\n",
    "#from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Acoustic Features alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data that was extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanF0Hz</th>\n",
       "      <th>stdevF0Hz</th>\n",
       "      <th>HNR</th>\n",
       "      <th>localJitter</th>\n",
       "      <th>localabsoluteJitter</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>apq5Shimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>231.250953</td>\n",
       "      <td>45.480073</td>\n",
       "      <td>16.525302</td>\n",
       "      <td>0.027821</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>0.011947</td>\n",
       "      <td>0.085180</td>\n",
       "      <td>0.922601</td>\n",
       "      <td>0.028993</td>\n",
       "      <td>0.049585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201.477299</td>\n",
       "      <td>14.236377</td>\n",
       "      <td>18.961886</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.075751</td>\n",
       "      <td>0.843053</td>\n",
       "      <td>0.021091</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209.285317</td>\n",
       "      <td>29.051820</td>\n",
       "      <td>12.840174</td>\n",
       "      <td>0.029609</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.013380</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.078756</td>\n",
       "      <td>0.813759</td>\n",
       "      <td>0.028769</td>\n",
       "      <td>0.037729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221.819604</td>\n",
       "      <td>31.556270</td>\n",
       "      <td>16.250999</td>\n",
       "      <td>0.027461</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.088893</td>\n",
       "      <td>0.941172</td>\n",
       "      <td>0.033405</td>\n",
       "      <td>0.054926</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199.675876</td>\n",
       "      <td>6.519508</td>\n",
       "      <td>14.700531</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.014263</td>\n",
       "      <td>0.016973</td>\n",
       "      <td>0.078197</td>\n",
       "      <td>0.684674</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.049475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     meanF0Hz  stdevF0Hz        HNR  localJitter  localabsoluteJitter  \\\n",
       "0  231.250953  45.480073  16.525302     0.027821             0.000121   \n",
       "1  201.477299  14.236377  18.961886     0.014327             0.000071   \n",
       "2  209.285317  29.051820  12.840174     0.029609             0.000143   \n",
       "3  221.819604  31.556270  16.250999     0.027461             0.000123   \n",
       "4  199.675876   6.519508  14.700531     0.026505             0.000134   \n",
       "\n",
       "   rapJitter  ppq5Jitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "0   0.011726    0.011947      0.085180        0.922601     0.028993   \n",
       "1   0.006781    0.007266      0.075751        0.843053     0.021091   \n",
       "2   0.013380    0.009806      0.078756        0.813759     0.028769   \n",
       "3   0.012908    0.015317      0.088893        0.941172     0.033405   \n",
       "4   0.014263    0.016973      0.078197        0.684674     0.030303   \n",
       "\n",
       "   apq5Shimmer  label  \n",
       "0     0.049585      0  \n",
       "1     0.034795      0  \n",
       "2     0.037729      0  \n",
       "3     0.054926      0  \n",
       "4     0.049475      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only acoustic features\n",
    "df = pd.read_csv(\"../MDVR_acoustic_features_chunks.csv\")\n",
    "#df = shuffle(df)\n",
    "#df.reset_index(inplace=True, drop=True)\n",
    "df.drop('voiceID', inplace = True, axis = 1)\n",
    "df['label'].value_counts()\n",
    "df = df.dropna()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808, 12)\n",
      "         meanF0Hz   stdevF0Hz         HNR  localJitter  localabsoluteJitter  \\\n",
      "count  808.000000  808.000000  808.000000   808.000000           808.000000   \n",
      "mean   179.784643   25.144978   13.091164     0.022284             0.000134   \n",
      "std     47.328209   17.955716    3.185657     0.011296             0.000075   \n",
      "min     92.441819    1.031706    2.928637     0.003969             0.000020   \n",
      "25%    143.268451   12.156897   10.916644     0.016458             0.000084   \n",
      "50%    182.170458   20.699903   12.878943     0.020723             0.000118   \n",
      "75%    206.928781   33.558460   15.171925     0.025975             0.000163   \n",
      "max    446.425562  143.864146   24.281832     0.124222             0.000672   \n",
      "\n",
      "        rapJitter  ppq5Jitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
      "count  808.000000  808.000000    808.000000      808.000000   808.000000   \n",
      "mean     0.009700    0.010928      0.090910        0.892078     0.033753   \n",
      "std      0.005744    0.007887      0.030945        0.245857     0.015745   \n",
      "min      0.001375    0.001447      0.030044        0.295598     0.010417   \n",
      "25%      0.006816    0.007517      0.071755        0.731631     0.024726   \n",
      "50%      0.008868    0.009873      0.087034        0.879952     0.030736   \n",
      "75%      0.011392    0.012395      0.103994        1.015841     0.039181   \n",
      "max      0.068436    0.139163      0.392267        2.324471     0.219708   \n",
      "\n",
      "       apq5Shimmer       label  \n",
      "count   808.000000  808.000000  \n",
      "mean      0.048662    0.419554  \n",
      "std       0.021141    0.493792  \n",
      "min       0.013275    0.000000  \n",
      "25%       0.035949    0.000000  \n",
      "50%       0.044928    0.000000  \n",
      "75%       0.056801    1.000000  \n",
      "max       0.268241    1.000000  \n",
      "label\n",
      "0    469\n",
      "1    339\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# shape\n",
    "print(df.shape)\n",
    "\n",
    "# descriptions\n",
    "print(df.describe())\n",
    "\n",
    "# class distribution\n",
    "print(df.groupby('label').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Check outliers Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histograms\n",
    "plt.figure(figsize=(30,30))\n",
    "#df_all.hist(figsize=(30,30))\n",
    "#plt.show()\n",
    "\n",
    "#box plot to check outliers\n",
    "cols = df.columns.drop('label')\n",
    "fig, ax = plt.subplots(4,3, figsize=(30,30))\n",
    "for i,t in enumerate(cols):\n",
    "    sns.boxplot(y=t, x= \"label\", data=df, ax=ax[i//3,i % 3])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.385187208443277e-05\n",
      "0.0001632567819324\n",
      "7.940490984796725e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanF0Hz</th>\n",
       "      <th>stdevF0Hz</th>\n",
       "      <th>HNR</th>\n",
       "      <th>localJitter</th>\n",
       "      <th>localabsoluteJitter</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>apq5Shimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>436.526891</td>\n",
       "      <td>49.426400</td>\n",
       "      <td>17.290929</td>\n",
       "      <td>0.124222</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.060130</td>\n",
       "      <td>0.059168</td>\n",
       "      <td>0.172404</td>\n",
       "      <td>1.416609</td>\n",
       "      <td>0.110298</td>\n",
       "      <td>0.043545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>105.624426</td>\n",
       "      <td>51.261106</td>\n",
       "      <td>14.181993</td>\n",
       "      <td>0.029655</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.015782</td>\n",
       "      <td>0.076714</td>\n",
       "      <td>0.849089</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>0.038751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>118.070067</td>\n",
       "      <td>9.636819</td>\n",
       "      <td>10.699646</td>\n",
       "      <td>0.033656</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>0.116175</td>\n",
       "      <td>1.141910</td>\n",
       "      <td>0.042764</td>\n",
       "      <td>0.058444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>122.925231</td>\n",
       "      <td>8.232303</td>\n",
       "      <td>14.867126</td>\n",
       "      <td>0.035302</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.018468</td>\n",
       "      <td>0.017671</td>\n",
       "      <td>0.082776</td>\n",
       "      <td>0.824150</td>\n",
       "      <td>0.028361</td>\n",
       "      <td>0.039233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>349.748554</td>\n",
       "      <td>85.982228</td>\n",
       "      <td>17.152853</td>\n",
       "      <td>0.092147</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.023419</td>\n",
       "      <td>0.131716</td>\n",
       "      <td>1.091028</td>\n",
       "      <td>0.054938</td>\n",
       "      <td>0.066203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>102.620464</td>\n",
       "      <td>13.859461</td>\n",
       "      <td>14.239776</td>\n",
       "      <td>0.029050</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.058137</td>\n",
       "      <td>0.567546</td>\n",
       "      <td>0.017158</td>\n",
       "      <td>0.024261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>105.173298</td>\n",
       "      <td>22.991949</td>\n",
       "      <td>14.099778</td>\n",
       "      <td>0.030932</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.013938</td>\n",
       "      <td>0.075083</td>\n",
       "      <td>0.740534</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.036424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>93.014171</td>\n",
       "      <td>9.442112</td>\n",
       "      <td>13.019803</td>\n",
       "      <td>0.027584</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.007696</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>0.074712</td>\n",
       "      <td>0.749408</td>\n",
       "      <td>0.025947</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>99.176686</td>\n",
       "      <td>17.190880</td>\n",
       "      <td>12.637254</td>\n",
       "      <td>0.030614</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.009385</td>\n",
       "      <td>0.015430</td>\n",
       "      <td>0.064547</td>\n",
       "      <td>0.654457</td>\n",
       "      <td>0.022477</td>\n",
       "      <td>0.030985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>110.706257</td>\n",
       "      <td>10.102389</td>\n",
       "      <td>9.890545</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.012094</td>\n",
       "      <td>0.014893</td>\n",
       "      <td>0.108543</td>\n",
       "      <td>1.029208</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>107.479975</td>\n",
       "      <td>11.002565</td>\n",
       "      <td>11.454369</td>\n",
       "      <td>0.033393</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.013343</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>0.127687</td>\n",
       "      <td>1.152064</td>\n",
       "      <td>0.049104</td>\n",
       "      <td>0.069177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>123.795859</td>\n",
       "      <td>4.038564</td>\n",
       "      <td>9.140992</td>\n",
       "      <td>0.038153</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.024121</td>\n",
       "      <td>0.031045</td>\n",
       "      <td>0.281851</td>\n",
       "      <td>2.002528</td>\n",
       "      <td>0.168703</td>\n",
       "      <td>0.268241</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>108.159900</td>\n",
       "      <td>9.921206</td>\n",
       "      <td>12.502560</td>\n",
       "      <td>0.034110</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.019009</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.965892</td>\n",
       "      <td>0.031810</td>\n",
       "      <td>0.056074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>106.789684</td>\n",
       "      <td>12.045635</td>\n",
       "      <td>9.164555</td>\n",
       "      <td>0.034269</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.014072</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>0.121578</td>\n",
       "      <td>1.197362</td>\n",
       "      <td>0.047388</td>\n",
       "      <td>0.058934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>117.240612</td>\n",
       "      <td>10.415259</td>\n",
       "      <td>6.506905</td>\n",
       "      <td>0.037748</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.017169</td>\n",
       "      <td>0.016130</td>\n",
       "      <td>0.129982</td>\n",
       "      <td>1.229469</td>\n",
       "      <td>0.066557</td>\n",
       "      <td>0.099828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>92.441819</td>\n",
       "      <td>8.923055</td>\n",
       "      <td>11.600720</td>\n",
       "      <td>0.030363</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.077221</td>\n",
       "      <td>0.760205</td>\n",
       "      <td>0.025019</td>\n",
       "      <td>0.038069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>114.727650</td>\n",
       "      <td>12.476625</td>\n",
       "      <td>11.093573</td>\n",
       "      <td>0.038840</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.019539</td>\n",
       "      <td>0.015149</td>\n",
       "      <td>0.108776</td>\n",
       "      <td>1.077431</td>\n",
       "      <td>0.048363</td>\n",
       "      <td>0.048369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>105.900786</td>\n",
       "      <td>6.950439</td>\n",
       "      <td>10.009566</td>\n",
       "      <td>0.035813</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.016435</td>\n",
       "      <td>0.123347</td>\n",
       "      <td>1.083617</td>\n",
       "      <td>0.045549</td>\n",
       "      <td>0.069448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>113.366607</td>\n",
       "      <td>13.061917</td>\n",
       "      <td>9.638827</td>\n",
       "      <td>0.039851</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.018446</td>\n",
       "      <td>0.019510</td>\n",
       "      <td>0.097406</td>\n",
       "      <td>0.925732</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>0.062130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>108.114987</td>\n",
       "      <td>4.067098</td>\n",
       "      <td>9.975785</td>\n",
       "      <td>0.038658</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.023709</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>0.127052</td>\n",
       "      <td>1.213692</td>\n",
       "      <td>0.048779</td>\n",
       "      <td>0.059405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>158.764324</td>\n",
       "      <td>28.761099</td>\n",
       "      <td>5.967175</td>\n",
       "      <td>0.056439</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>0.008491</td>\n",
       "      <td>0.147794</td>\n",
       "      <td>1.221955</td>\n",
       "      <td>0.076565</td>\n",
       "      <td>0.105992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>115.579189</td>\n",
       "      <td>66.926741</td>\n",
       "      <td>11.570552</td>\n",
       "      <td>0.040818</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.019452</td>\n",
       "      <td>0.022387</td>\n",
       "      <td>0.110820</td>\n",
       "      <td>1.105300</td>\n",
       "      <td>0.046892</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>94.704772</td>\n",
       "      <td>18.770048</td>\n",
       "      <td>11.292347</td>\n",
       "      <td>0.035065</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>0.017610</td>\n",
       "      <td>0.084445</td>\n",
       "      <td>0.828761</td>\n",
       "      <td>0.024832</td>\n",
       "      <td>0.041062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>102.408318</td>\n",
       "      <td>9.718174</td>\n",
       "      <td>10.665477</td>\n",
       "      <td>0.038931</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.016282</td>\n",
       "      <td>0.026134</td>\n",
       "      <td>0.129739</td>\n",
       "      <td>1.237910</td>\n",
       "      <td>0.049775</td>\n",
       "      <td>0.074508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>105.058732</td>\n",
       "      <td>10.513483</td>\n",
       "      <td>5.497981</td>\n",
       "      <td>0.040219</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>0.012351</td>\n",
       "      <td>0.149535</td>\n",
       "      <td>1.291920</td>\n",
       "      <td>0.043209</td>\n",
       "      <td>0.052296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>109.695065</td>\n",
       "      <td>9.657805</td>\n",
       "      <td>7.741675</td>\n",
       "      <td>0.045696</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.021559</td>\n",
       "      <td>0.160831</td>\n",
       "      <td>1.433544</td>\n",
       "      <td>0.053934</td>\n",
       "      <td>0.102092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>97.565199</td>\n",
       "      <td>9.385452</td>\n",
       "      <td>11.190016</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.016316</td>\n",
       "      <td>0.019654</td>\n",
       "      <td>0.110194</td>\n",
       "      <td>0.987060</td>\n",
       "      <td>0.034662</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>101.547701</td>\n",
       "      <td>10.209203</td>\n",
       "      <td>8.811301</td>\n",
       "      <td>0.046944</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.021961</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>0.138575</td>\n",
       "      <td>1.294185</td>\n",
       "      <td>0.061351</td>\n",
       "      <td>0.085327</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>94.540019</td>\n",
       "      <td>2.352628</td>\n",
       "      <td>5.628797</td>\n",
       "      <td>0.048927</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.029446</td>\n",
       "      <td>0.021755</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>1.542377</td>\n",
       "      <td>0.093695</td>\n",
       "      <td>0.090756</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>125.587205</td>\n",
       "      <td>8.962313</td>\n",
       "      <td>12.439040</td>\n",
       "      <td>0.067079</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.038959</td>\n",
       "      <td>0.034317</td>\n",
       "      <td>0.147005</td>\n",
       "      <td>1.155790</td>\n",
       "      <td>0.039582</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>217.784077</td>\n",
       "      <td>37.076851</td>\n",
       "      <td>11.670683</td>\n",
       "      <td>0.115380</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.068436</td>\n",
       "      <td>0.089811</td>\n",
       "      <td>0.202285</td>\n",
       "      <td>1.826999</td>\n",
       "      <td>0.103223</td>\n",
       "      <td>0.155980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>182.826527</td>\n",
       "      <td>31.102973</td>\n",
       "      <td>14.012335</td>\n",
       "      <td>0.122154</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.045864</td>\n",
       "      <td>0.139163</td>\n",
       "      <td>0.127022</td>\n",
       "      <td>1.100419</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.077514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>105.610168</td>\n",
       "      <td>9.351154</td>\n",
       "      <td>4.920061</td>\n",
       "      <td>0.071081</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.040999</td>\n",
       "      <td>0.026774</td>\n",
       "      <td>0.149796</td>\n",
       "      <td>1.438537</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.053734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       meanF0Hz  stdevF0Hz        HNR  localJitter  localabsoluteJitter  \\\n",
       "420  436.526891  49.426400  17.290929     0.124222             0.000283   \n",
       "695  105.624426  51.261106  14.181993     0.029655             0.000285   \n",
       "539  118.070067   9.636819  10.699646     0.033656             0.000286   \n",
       "704  122.925231   8.232303  14.867126     0.035302             0.000287   \n",
       "186  349.748554  85.982228  17.152853     0.092147             0.000288   \n",
       "692  102.620464  13.859461  14.239776     0.029050             0.000288   \n",
       "689  105.173298  22.991949  14.099778     0.030932             0.000296   \n",
       "696   93.014171   9.442112  13.019803     0.027584             0.000297   \n",
       "697   99.176686  17.190880  12.637254     0.030614             0.000309   \n",
       "515  110.706257  10.102389   9.890545     0.034401             0.000311   \n",
       "506  107.479975  11.002565  11.454369     0.033393             0.000311   \n",
       "446  123.795859   4.038564   9.140992     0.038153             0.000312   \n",
       "517  108.159900   9.921206  12.502560     0.034110             0.000317   \n",
       "526  106.789684  12.045635   9.164555     0.034269             0.000320   \n",
       "550  117.240612  10.415259   6.506905     0.037748             0.000322   \n",
       "690   92.441819   8.923055  11.600720     0.030363             0.000331   \n",
       "543  114.727650  12.476625  11.093573     0.038840             0.000335   \n",
       "518  105.900786   6.950439  10.009566     0.035813             0.000337   \n",
       "511  113.366607  13.061917   9.638827     0.039851             0.000351   \n",
       "533  108.114987   4.067098   9.975785     0.038658             0.000357   \n",
       "443  158.764324  28.761099   5.967175     0.056439             0.000358   \n",
       "509  115.579189  66.926741  11.570552     0.040818             0.000368   \n",
       "698   94.704772  18.770048  11.292347     0.035065             0.000375   \n",
       "520  102.408318   9.718174  10.665477     0.038931             0.000383   \n",
       "521  105.058732  10.513483   5.497981     0.040219             0.000384   \n",
       "504  109.695065   9.657805   7.741675     0.045696             0.000415   \n",
       "688   97.565199   9.385452  11.190016     0.042722             0.000441   \n",
       "510  101.547701  10.209203   8.811301     0.046944             0.000462   \n",
       "524   94.540019   2.352628   5.628797     0.048927             0.000511   \n",
       "672  125.587205   8.962313  12.439040     0.067079             0.000536   \n",
       "709  217.784077  37.076851  11.670683     0.115380             0.000539   \n",
       "706  182.826527  31.102973  14.012335     0.122154             0.000669   \n",
       "525  105.610168   9.351154   4.920061     0.071081             0.000672   \n",
       "\n",
       "     rapJitter  ppq5Jitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "420   0.060130    0.059168      0.172404        1.416609     0.110298   \n",
       "695   0.010280    0.015782      0.076714        0.849089     0.022786   \n",
       "539   0.014336    0.015074      0.116175        1.141910     0.042764   \n",
       "704   0.018468    0.017671      0.082776        0.824150     0.028361   \n",
       "186   0.046544    0.023419      0.131716        1.091028     0.054938   \n",
       "692   0.009480    0.009917      0.058137        0.567546     0.017158   \n",
       "689   0.010441    0.013938      0.075083        0.740534     0.021738   \n",
       "696   0.007696    0.013530      0.074712        0.749408     0.025947   \n",
       "697   0.009385    0.015430      0.064547        0.654457     0.022477   \n",
       "515   0.012094    0.014893      0.108543        1.029208     0.025679   \n",
       "506   0.013343    0.015433      0.127687        1.152064     0.049104   \n",
       "446   0.024121    0.031045      0.281851        2.002528     0.168703   \n",
       "517   0.015271    0.019009      0.096100        0.965892     0.031810   \n",
       "526   0.014072    0.014479      0.121578        1.197362     0.047388   \n",
       "550   0.017169    0.016130      0.129982        1.229469     0.066557   \n",
       "690   0.009014    0.012686      0.077221        0.760205     0.025019   \n",
       "543   0.019539    0.015149      0.108776        1.077431     0.048363   \n",
       "518   0.012726    0.016435      0.123347        1.083617     0.045549   \n",
       "511   0.018446    0.019510      0.097406        0.925732     0.040767   \n",
       "533   0.023709    0.019469      0.127052        1.213692     0.048779   \n",
       "443   0.015832    0.008491      0.147794        1.221955     0.076565   \n",
       "509   0.019452    0.022387      0.110820        1.105300     0.046892   \n",
       "698   0.010380    0.017610      0.084445        0.828761     0.024832   \n",
       "520   0.016282    0.026134      0.129739        1.237910     0.049775   \n",
       "521   0.018856    0.012351      0.149535        1.291920     0.043209   \n",
       "504   0.019851    0.021559      0.160831        1.433544     0.053934   \n",
       "688   0.016316    0.019654      0.110194        0.987060     0.034662   \n",
       "510   0.021961    0.022543      0.138575        1.294185     0.061351   \n",
       "524   0.029446    0.021755      0.162813        1.542377     0.093695   \n",
       "672   0.038959    0.034317      0.147005        1.155790     0.039582   \n",
       "709   0.068436    0.089811      0.202285        1.826999     0.103223   \n",
       "706   0.045864    0.139163      0.127022        1.100419     0.068303   \n",
       "525   0.040999    0.026774      0.149796        1.438537     0.084746   \n",
       "\n",
       "     apq5Shimmer  label  \n",
       "420     0.043545      0  \n",
       "695     0.038751      1  \n",
       "539     0.058444      1  \n",
       "704     0.039233      1  \n",
       "186     0.066203      0  \n",
       "692     0.024261      1  \n",
       "689     0.036424      1  \n",
       "696     0.041783      1  \n",
       "697     0.030985      1  \n",
       "515     0.051402      1  \n",
       "506     0.069177      1  \n",
       "446     0.268241      0  \n",
       "517     0.056074      1  \n",
       "526     0.058934      1  \n",
       "550     0.099828      1  \n",
       "690     0.038069      1  \n",
       "543     0.048369      1  \n",
       "518     0.069448      1  \n",
       "511     0.062130      1  \n",
       "533     0.059405      1  \n",
       "443     0.105992      0  \n",
       "509     0.065678      1  \n",
       "698     0.041062      1  \n",
       "520     0.074508      1  \n",
       "521     0.052296      1  \n",
       "504     0.102092      1  \n",
       "688     0.057010      1  \n",
       "510     0.085327      1  \n",
       "524     0.090756      1  \n",
       "672     0.067725      1  \n",
       "709     0.155980      1  \n",
       "706     0.077514      1  \n",
       "525     0.053734      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jitter = df.sort_values(\"localabsoluteJitter\")\n",
    "Q1=df_jitter['localabsoluteJitter'].quantile(0.25)\n",
    "Q3=df_jitter['localabsoluteJitter'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "Lower_Whisker = Q1 - 1.5*IQR\n",
    "Upper_Whisker = Q3 + 1.5*IQR\n",
    "\n",
    "df_1 = df_jitter[df_jitter['localabsoluteJitter'] > Upper_Whisker]\n",
    "df_1.append(df_jitter[df_jitter['localabsoluteJitter'] < Lower_Whisker])\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#separate dependent and independent variable for acoustic features only\n",
    "X = df.iloc[:, :-1]\n",
    "df_X = df.iloc[:, :-1].values\n",
    "df_Y = df.iloc[:,-1].values\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.154648</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>0.787054</td>\n",
       "      <td>0.045324</td>\n",
       "      <td>0.065681</td>\n",
       "      <td>0.049453</td>\n",
       "      <td>0.027517</td>\n",
       "      <td>0.117502</td>\n",
       "      <td>0.140266</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>0.150775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.359383</td>\n",
       "      <td>0.138233</td>\n",
       "      <td>0.527694</td>\n",
       "      <td>0.146881</td>\n",
       "      <td>0.118045</td>\n",
       "      <td>0.124615</td>\n",
       "      <td>0.059992</td>\n",
       "      <td>0.208179</td>\n",
       "      <td>0.392777</td>\n",
       "      <td>0.114899</td>\n",
       "      <td>0.090757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266854</td>\n",
       "      <td>0.158108</td>\n",
       "      <td>0.644371</td>\n",
       "      <td>0.082157</td>\n",
       "      <td>0.082628</td>\n",
       "      <td>0.042779</td>\n",
       "      <td>0.024084</td>\n",
       "      <td>0.126883</td>\n",
       "      <td>0.194980</td>\n",
       "      <td>0.143321</td>\n",
       "      <td>0.118694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.989576</td>\n",
       "      <td>0.300506</td>\n",
       "      <td>0.481678</td>\n",
       "      <td>0.665703</td>\n",
       "      <td>0.253371</td>\n",
       "      <td>0.560716</td>\n",
       "      <td>0.625995</td>\n",
       "      <td>0.457918</td>\n",
       "      <td>0.610710</td>\n",
       "      <td>0.501694</td>\n",
       "      <td>0.186052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.492156</td>\n",
       "      <td>0.149232</td>\n",
       "      <td>0.118242</td>\n",
       "      <td>0.243871</td>\n",
       "      <td>0.159527</td>\n",
       "      <td>0.185294</td>\n",
       "      <td>0.106677</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.515990</td>\n",
       "      <td>0.199637</td>\n",
       "      <td>0.107838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0.274639</td>\n",
       "      <td>0.178036</td>\n",
       "      <td>0.705424</td>\n",
       "      <td>0.134340</td>\n",
       "      <td>0.129217</td>\n",
       "      <td>0.107191</td>\n",
       "      <td>0.050935</td>\n",
       "      <td>0.235467</td>\n",
       "      <td>0.330539</td>\n",
       "      <td>0.297956</td>\n",
       "      <td>0.216335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0.179311</td>\n",
       "      <td>0.269568</td>\n",
       "      <td>0.364460</td>\n",
       "      <td>0.209696</td>\n",
       "      <td>0.250382</td>\n",
       "      <td>0.185847</td>\n",
       "      <td>0.066050</td>\n",
       "      <td>0.194868</td>\n",
       "      <td>0.300701</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.150002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0.079025</td>\n",
       "      <td>0.059888</td>\n",
       "      <td>0.477472</td>\n",
       "      <td>0.110413</td>\n",
       "      <td>0.186168</td>\n",
       "      <td>0.065168</td>\n",
       "      <td>0.034257</td>\n",
       "      <td>0.129020</td>\n",
       "      <td>0.184829</td>\n",
       "      <td>0.147221</td>\n",
       "      <td>0.131005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.144036</td>\n",
       "      <td>0.093019</td>\n",
       "      <td>0.734637</td>\n",
       "      <td>0.153207</td>\n",
       "      <td>0.206145</td>\n",
       "      <td>0.132728</td>\n",
       "      <td>0.085890</td>\n",
       "      <td>0.159813</td>\n",
       "      <td>0.276885</td>\n",
       "      <td>0.171980</td>\n",
       "      <td>0.125916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.434480</td>\n",
       "      <td>0.223325</td>\n",
       "      <td>0.477692</td>\n",
       "      <td>0.111736</td>\n",
       "      <td>0.077603</td>\n",
       "      <td>0.152219</td>\n",
       "      <td>0.217858</td>\n",
       "      <td>0.146207</td>\n",
       "      <td>0.119999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>565 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.154648  0.012578  0.787054  0.045324  0.065681  0.049453  0.027517   \n",
       "1    0.359383  0.138233  0.527694  0.146881  0.118045  0.124615  0.059992   \n",
       "2    0.266854  0.158108  0.644371  0.082157  0.082628  0.042779  0.024084   \n",
       "3    0.989576  0.300506  0.481678  0.665703  0.253371  0.560716  0.625995   \n",
       "4    0.492156  0.149232  0.118242  0.243871  0.159527  0.185294  0.106677   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "560  0.274639  0.178036  0.705424  0.134340  0.129217  0.107191  0.050935   \n",
       "561  0.179311  0.269568  0.364460  0.209696  0.250382  0.185847  0.066050   \n",
       "562  0.079025  0.059888  0.477472  0.110413  0.186168  0.065168  0.034257   \n",
       "563  0.144036  0.093019  0.734637  0.153207  0.206145  0.132728  0.085890   \n",
       "564  0.000000  0.055249  0.434480  0.223325  0.477692  0.111736  0.077603   \n",
       "\n",
       "           7         8         9         10  \n",
       "0    0.117502  0.140266  0.171648  0.150775  \n",
       "1    0.208179  0.392777  0.114899  0.090757  \n",
       "2    0.126883  0.194980  0.143321  0.118694  \n",
       "3    0.457918  0.610710  0.501694  0.186052  \n",
       "4    0.324000  0.515990  0.199637  0.107838  \n",
       "..        ...       ...       ...       ...  \n",
       "560  0.235467  0.330539  0.297956  0.216335  \n",
       "561  0.194868  0.300701  0.267327  0.150002  \n",
       "562  0.129020  0.184829  0.147221  0.131005  \n",
       "563  0.159813  0.276885  0.171980  0.125916  \n",
       "564  0.152219  0.217858  0.146207  0.119999  \n",
       "\n",
       "[565 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale for acoustic features\n",
    "#sc = StandardScaler()\n",
    "sc = MinMaxScaler()\n",
    "#sc = RobustScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****************KNN Experiments******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.60493827160494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.85      0.77       137\n",
      "           1       0.74      0.54      0.62       106\n",
      "\n",
      "    accuracy                           0.72       243\n",
      "   macro avg       0.72      0.70      0.70       243\n",
      "weighted avg       0.72      0.72      0.71       243\n",
      "\n",
      "0.6958752237983749\n",
      "[0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0\n",
      " 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 1 0\n",
      " 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "[[117  20]\n",
      " [ 49  57]]\n"
     ]
    }
   ],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#KNN\n",
    "model_knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "accuracy_knn = ((conf_matrix_knn[0,0] + conf_matrix_knn[1,1])/(conf_matrix_knn[0,0] +conf_matrix_knn[0,1]+conf_matrix_knn[1,0]+conf_matrix_knn[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn))\n",
    "\n",
    "print(y_pred_knn)\n",
    "\n",
    "print(conf_matrix_knn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best leaf_size: 1\n",
      "Best p: 1\n",
      "Best n_neighbors: 12\n",
      "Best Score: 0.800062656641604\n",
      "Best Hyperparameters: {'leaf_size': 1, 'n_neighbors': 12, 'p': 1}\n",
      "{'leaf_size': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], 'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'p': [1, 2]}\n",
      "72.42798353909465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.77       137\n",
      "           1       0.73      0.58      0.65       106\n",
      "\n",
      "    accuracy                           0.72       243\n",
      "   macro avg       0.73      0.71      0.71       243\n",
      "weighted avg       0.73      0.72      0.72       243\n",
      "\n",
      "0.707443878253684\n",
      "[0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0\n",
      " 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "[1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1\n",
      " 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
      " 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1\n",
      " 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 0\n",
      " 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0]\n",
      "[[115  22]\n",
      " [ 45  61]]\n"
     ]
    }
   ],
   "source": [
    "########Hyperparameter tuning for KNN####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,20)) #neighbours must be < number of samples (22)\n",
    "p=[1,2]\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.42798353909465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.77       137\n",
      "           1       0.73      0.58      0.65       106\n",
      "\n",
      "    accuracy                           0.72       243\n",
      "   macro avg       0.73      0.71      0.71       243\n",
      "weighted avg       0.73      0.72      0.72       243\n",
      "\n",
      "0.707443878253684\n",
      "[0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0\n",
      " 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "[1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1\n",
      " 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
      " 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1\n",
      " 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 0\n",
      " 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0]\n",
      "[[115  22]\n",
      " [ 45  61]]\n"
     ]
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors = 12, p = 1, leaf_size = 1)\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "accuracy_knn = ((conf_matrix_knn[0,0] + conf_matrix_knn[1,1])/(conf_matrix_knn[0,0] +conf_matrix_knn[0,1]+conf_matrix_knn[1,0]+conf_matrix_knn[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn))\n",
    "\n",
    "print(y_pred_knn)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn)\n",
    "\n",
    "####################using the acoustic + MFCC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method -IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.723 (0.254)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0\n",
      " 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0\n",
      " 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix for KNN using k-fold (leave one out)\n",
      "[[377  92]\n",
      " [130 209]]\n",
      "72.52475247524752\n"
     ]
    }
   ],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "model_knn_kfold = KNeighborsClassifier(n_neighbors = 9, p =1, leaf_size = 1)\n",
    "y_pred_kfold_knn = cross_val_predict(model_knn_kfold, df_X, df_Y, cv=k_fold)\n",
    "\n",
    "scores = cross_val_score(model_knn_kfold, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_knn)\n",
    "conf_matrix_knn_kfold = confusion_matrix(df_Y, y_pred_kfold_knn)\n",
    "print(\"Confusion Matrix for KNN using k-fold (leave one out)\")\n",
    "print(conf_matrix_knn_kfold)\n",
    "\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/(conf_matrix_knn_kfold[0,0] +conf_matrix_knn_kfold[0,1]+conf_matrix_knn_kfold[1,0]+conf_matrix_knn_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divided into 4 parts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "<ipython-input-23-1ca028e49b9b>:74: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Kfold Evaluation for MDVR-KCL Dataset - Classification Accuracy\n",
      "    Iteration     fold 1     fold 2     fold 3     fold 4  mean accuracy\n",
      "0           1  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "1           2  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "2           3  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "3           4  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "4           5  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "5           6  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "6           7  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "7           8  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "8           9  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "9          10  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "10         11  60.891089  64.356436  61.386139  34.653465      55.321782\n",
      "KNN Kfold Evaluation for MDVR-KCL Dataset - Specificity\n",
      "    Loops     fold 1     fold 2  fold 3  fold 4  mean specificity\n",
      "0       1  60.891089  64.356436    60.0     NaN               NaN\n",
      "1       2  60.891089  64.356436    60.0     NaN               NaN\n",
      "2       3  60.891089  64.356436    60.0     NaN               NaN\n",
      "3       4  60.891089  64.356436    60.0     NaN               NaN\n",
      "4       5  60.891089  64.356436    60.0     NaN               NaN\n",
      "5       6  60.891089  64.356436    60.0     NaN               NaN\n",
      "6       7  60.891089  64.356436    60.0     NaN               NaN\n",
      "7       8  60.891089  64.356436    60.0     NaN               NaN\n",
      "8       9  60.891089  64.356436    60.0     NaN               NaN\n",
      "9      10  60.891089  64.356436    60.0     NaN               NaN\n",
      "10     11  60.891089  64.356436    60.0     NaN               NaN\n",
      "KNN Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\n",
      "    Loops  fold 1  fold 2     fold 3     fold 4  mean sensitivity/recall\n",
      "0       1     NaN     NaN  62.043796  34.653465                      NaN\n",
      "1       2     NaN     NaN  62.043796  34.653465                      NaN\n",
      "2       3     NaN     NaN  62.043796  34.653465                      NaN\n",
      "3       4     NaN     NaN  62.043796  34.653465                      NaN\n",
      "4       5     NaN     NaN  62.043796  34.653465                      NaN\n",
      "5       6     NaN     NaN  62.043796  34.653465                      NaN\n",
      "6       7     NaN     NaN  62.043796  34.653465                      NaN\n",
      "7       8     NaN     NaN  62.043796  34.653465                      NaN\n",
      "8       9     NaN     NaN  62.043796  34.653465                      NaN\n",
      "9      10     NaN     NaN  62.043796  34.653465                      NaN\n",
      "10     11     NaN     NaN  62.043796  34.653465                      NaN\n",
      "KNN Kfold Evaluation for MDVR-KCL Dataset - Precision\n",
      "    Loops  fold 1  fold 2     fold 3  fold 4  mean precision\n",
      "0       1     0.0     0.0  76.576577   100.0       44.144144\n",
      "1       2     0.0     0.0  76.576577   100.0       44.144144\n",
      "2       3     0.0     0.0  76.576577   100.0       44.144144\n",
      "3       4     0.0     0.0  76.576577   100.0       44.144144\n",
      "4       5     0.0     0.0  76.576577   100.0       44.144144\n",
      "5       6     0.0     0.0  76.576577   100.0       44.144144\n",
      "6       7     0.0     0.0  76.576577   100.0       44.144144\n",
      "7       8     0.0     0.0  76.576577   100.0       44.144144\n",
      "8       9     0.0     0.0  76.576577   100.0       44.144144\n",
      "9      10     0.0     0.0  76.576577   100.0       44.144144\n",
      "10     11     0.0     0.0  76.576577   100.0       44.144144\n",
      "KNN Kfold Evaluation for MDVR-KCL Dataset - F1 score\n",
      "    Loops  fold 1  fold 2     fold 3     fold 4  mean f1 score\n",
      "0       1     NaN     NaN  68.548387  51.470588            NaN\n",
      "1       2     NaN     NaN  68.548387  51.470588            NaN\n",
      "2       3     NaN     NaN  68.548387  51.470588            NaN\n",
      "3       4     NaN     NaN  68.548387  51.470588            NaN\n",
      "4       5     NaN     NaN  68.548387  51.470588            NaN\n",
      "5       6     NaN     NaN  68.548387  51.470588            NaN\n",
      "6       7     NaN     NaN  68.548387  51.470588            NaN\n",
      "7       8     NaN     NaN  68.548387  51.470588            NaN\n",
      "8       9     NaN     NaN  68.548387  51.470588            NaN\n",
      "9      10     NaN     NaN  68.548387  51.470588            NaN\n",
      "10     11     NaN     NaN  68.548387  51.470588            NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-1ca028e49b9b>:75: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity_knn_kfold = (TN/(TN + FP)) * 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>60.891089</td>\n",
       "      <td>64.356436</td>\n",
       "      <td>61.386139</td>\n",
       "      <td>34.653465</td>\n",
       "      <td>55.321782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iteration     fold 1     fold 2     fold 3     fold 4  mean accuracy\n",
       "0           1  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "1           2  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "2           3  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "3           4  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "4           5  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "5           6  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "6           7  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "7           8  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "8           9  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "9          10  60.891089  64.356436  61.386139  34.653465      55.321782\n",
       "10         11  60.891089  64.356436  61.386139  34.653465      55.321782"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_list = []\n",
    "k_specificity = []\n",
    "k_sensitivity = []\n",
    "k_precision = []\n",
    "k_f1 = []\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "        #print(Ytrain_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        model_knn_new = KNeighborsClassifier(n_neighbors = 9, p =1, leaf_size = 1)\n",
    "        model_knn_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        y_pred_knn_new = model_knn_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_knn_new)\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "        \n",
    "        #total = (conf_matrix_knn_kfold[0,0] +conf_matrix_knn_kfold[0,1]+conf_matrix_knn_kfold[1,0]+conf_matrix_knn_kfold[1,1])\n",
    "\n",
    "        #accuracy_knn_kfold = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/total)*100\n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) * 100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "        #sensitivity_knn_kfold = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/total)*100\n",
    "\n",
    "        #print(\"Confusion Matrix:\\n \", conf_matrix_knn_kfold)\n",
    "        #print(\"Accuracy \", accuracy_knn_kfold)\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "        \n",
    "        #plot roc only for the first iteration\n",
    "        #if i == 1:\n",
    "           # metrics.plot_roc_curve(model_knn_new, Xtest_kfold, Ytest_kfold,name='ROC fold {}'.format(j),\n",
    "                     #    alpha=0.3, lw=1, ax=ax)\n",
    "        \n",
    "        \n",
    "    average = row.append(total/parts)\n",
    "    \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_list.append(row)\n",
    "    k_specificity.append(row_specificity)\n",
    "    k_sensitivity.append(row_sensitivity)\n",
    "    k_precision.append(row_precision)\n",
    "    k_f1.append(row_f1)\n",
    "    \n",
    "      \n",
    "k_list = pd.DataFrame(k_list, columns=['Iteration','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - Classification Accuracy\")\n",
    "print(k_list)\n",
    "    \n",
    "\n",
    "k_specificity = pd.DataFrame(k_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_specificity)\n",
    "\n",
    "k_sensitivity = pd.DataFrame(k_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_sensitivity)\n",
    "\n",
    "k_precision = pd.DataFrame(k_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_precision)\n",
    "\n",
    "k_f1 = pd.DataFrame(k_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_f1)\n",
    "    \n",
    "k_list\n",
    "    \n",
    "#print(df.sample(n=7))\n",
    "#print(df)\n",
    "\n",
    "#x = df.take(np.random.permutation(len(df))[:4])\n",
    "#x= df.sample(n=7)\n",
    "#print(x)\n",
    "#print(df.drop(x))\n",
    "\n",
    "#drop_indices = np.random.choice(df.index, 4, replace=False)\n",
    "#df_subset = df.drop(drop_indices)\n",
    "#print(drop_indices)\n",
    "#print(df_subset)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Decision Tree Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#Decision Tree\n",
    "import matplotlib.pyplot as plt\n",
    "model_dt = tree.DecisionTreeClassifier()\n",
    "model_dt = model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_dt.predict(X_test)\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "accuracy_knn = ((conf_matrix_knn[0,0] + conf_matrix_knn[1,1])/(conf_matrix_knn[0,0] +conf_matrix_knn[0,1]+conf_matrix_knn[1,0]+conf_matrix_knn[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn))\n",
    "\n",
    "print(y_pred_knn)\n",
    "\n",
    "print(conf_matrix_knn)\n",
    "\n",
    "plt.figure(figsize=(24,14))\n",
    "tree.plot_tree(model_dt, filled=True, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for KNN####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "max_depth = list(range(1,10))\n",
    "min_samples_split = list(range(2,10)) #neighbours must be < number of samples (22)\n",
    "min_samples_leaf = list(range(1,5))\n",
    "criterion=['gini','entropy']\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, criterion = criterion)\n",
    "#Create new KNN object\n",
    "dt_2 = tree.DecisionTreeClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(dt_2, hyperparameters, refit=True)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (Decision Tree)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model_dt = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_dt = tree.DecisionTreeClassifier(max_depth = 1, criterion='gini',min_samples_leaf=1, min_samples_split=2 )\n",
    "model_dt = model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_dt_2 = confusion_matrix(y_test, y_pred_dt_2)\n",
    "\n",
    "accuracy_dt_2 = ((conf_matrix_dt_2[0,0] + conf_matrix_dt_2[1,1])/(conf_matrix_dt_2[0,0] +conf_matrix_dt_2[0,1]+conf_matrix_dt_2[1,0]+conf_matrix_dt_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_dt_2)\n",
    "print(accuracy_dt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method - IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "#model_knn_kfold = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_dt = tree.DecisionTreeClassifier(max_depth = 1, criterion='gini',min_samples_leaf=1, min_samples_split=2 )\n",
    "y_pred_kfold_knn = cross_val_predict(model_dt, df_X, df_Y, cv=k_fold)\n",
    "scores = cross_val_score(model_dt, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_knn)\n",
    "conf_matrix_knn_kfold = confusion_matrix(df_Y, y_pred_kfold_knn)\n",
    "print(\"Confusion Matrix for Dt using k-fold (leave one out)\")\n",
    "print(conf_matrix_knn_kfold)\n",
    "\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/(conf_matrix_knn_kfold[0,0] +conf_matrix_knn_kfold[0,1]+conf_matrix_knn_kfold[1,0]+conf_matrix_knn_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_dt_list = []\n",
    "k_dt_specificity = []\n",
    "k_dt_sensitivity = []\n",
    "k_dt_precision = []\n",
    "k_dt_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        #model_knn_new = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "        model_dt_new = tree.DecisionTreeClassifier(max_depth = 1, criterion='gini',min_samples_leaf=1, min_samples_split=2 )\n",
    "        model_dt_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        y_pred_dt_new = model_dt_new.predict(Xtest_kfold)\n",
    "\n",
    "        #conf_matrix_dt_kfold = confusion_matrix(Ytest_kfold, y_pred_dt_new)\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_dt_new)\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_dt_list.append(row)\n",
    "    k_dt_specificity.append(row_specificity)\n",
    "    k_dt_sensitivity.append(row_sensitivity)\n",
    "    k_dt_precision.append(row_precision)\n",
    "    k_dt_f1.append(row_f1)\n",
    "    \n",
    "k_dt_list = pd.DataFrame(k_dt_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_dt_list)    \n",
    "\n",
    "k_dt_specificity = pd.DataFrame(k_dt_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_dt_specificity)\n",
    "\n",
    "k_dt_sensitivity = pd.DataFrame(k_dt_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_dt_sensitivity)\n",
    "\n",
    "k_dt_precision = pd.DataFrame(k_dt_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_dt_precision)\n",
    "\n",
    "k_dt_f1 = pd.DataFrame(k_dt_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_dt_f1)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************SVM Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#Decision Tree\n",
    "import matplotlib.pyplot as plt\n",
    "model_svm = svm.SVC()\n",
    "model_svm = model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = model_dt.predict(X_test)\n",
    "\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "accuracy_svm = ((conf_matrix_svm[0,0] + conf_matrix_svm[1,1])/(conf_matrix_svm[0,0] +conf_matrix_svm[0,1]+conf_matrix_svm[1,0]+conf_matrix_svm[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_svm)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_svm))\n",
    "\n",
    "print(y_pred_svm)\n",
    "\n",
    "print(conf_matrix_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "C = [0.1, 1, 10, 100, 1000]\n",
    "gamma = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "kernel = ['rbf']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C=C, gamma=gamma, kernel=kernel)\n",
    "#Create new KNN object\n",
    "svm2 = svm.SVC()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(svm2, hyperparameters, refit=True)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (SVM)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_svm = svm.SVC(C = 10, gamma=0.1,kernel='rbf')\n",
    "model_svm = model_svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_svm_2 = confusion_matrix(y_test, y_pred_svm_2)\n",
    "\n",
    "accuracy_svm_2 = ((conf_matrix_svm_2[0,0] + conf_matrix_svm_2[1,1])/(conf_matrix_svm_2[0,0] +conf_matrix_svm_2[0,1]+conf_matrix_svm_2[1,0]+conf_matrix_svm_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_svm_2)\n",
    "print(accuracy_svm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (SVM) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "#model_knn_kfold = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_svm = svm.SVC(C = 10, gamma=0.1,kernel='rbf')\n",
    "y_pred_kfold_svm = cross_val_predict(model_svm, df_X, df_Y, cv=k_fold)\n",
    "scores = cross_val_score(model_svm, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_knn)\n",
    "conf_matrix_svm_kfold = confusion_matrix(df_Y, y_pred_kfold_svm)\n",
    "print(\"Confusion Matrix for Dt using k-fold (leave one out)\")\n",
    "print(conf_matrix_svm_kfold)\n",
    "\n",
    "\n",
    "accuracy_svm_2 = ((conf_matrix_svm_kfold[0,0] + conf_matrix_svm_kfold[1,1])/(conf_matrix_svm_kfold[0,0] +conf_matrix_svm_kfold[0,1]+conf_matrix_svm_kfold[1,0]+conf_matrix_svm_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_svm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_svm_list = []\n",
    "k_svm_specificity = []\n",
    "k_svm_sensitivity = []\n",
    "k_svm_precision = []\n",
    "k_svm_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        \n",
    "        model_svm_new = svm.SVC(C = 10, gamma=0.1,kernel='rbf')\n",
    "        model_svm_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        y_pred_svm_new = model_svm_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_svm_new)\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_svm_list.append(row)\n",
    "    k_svm_specificity.append(row_specificity)\n",
    "    k_svm_sensitivity.append(row_sensitivity)\n",
    "    k_svm_precision.append(row_precision)\n",
    "    k_svm_f1.append(row_f1)\n",
    "    \n",
    "k_svm_list = pd.DataFrame(k_svm_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_svm_list)    \n",
    "\n",
    "k_svm_specificity = pd.DataFrame(k_svm_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_svm_specificity)\n",
    "\n",
    "k_svm_sensitivity = pd.DataFrame(k_svm_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_svm_sensitivity)\n",
    "\n",
    "k_svm_precision = pd.DataFrame(k_svm_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_svm_precision)\n",
    "\n",
    "k_svm_f1 = pd.DataFrame(k_svm_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_svm_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Naive Bayes Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "model_nb = GaussianNB()\n",
    "model_nb = model_nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_nb = model_nb.predict(X_test)\n",
    "\n",
    "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "accuracy_nb = ((conf_matrix_nb[0,0] + conf_matrix_nb[1,1])/(conf_matrix_nb[0,0] +conf_matrix_nb[0,1]+conf_matrix_nb[1,0]+conf_matrix_nb[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_nb)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_nb))\n",
    "\n",
    "print(y_pred_nb)\n",
    "\n",
    "print(conf_matrix_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB doesnt have important parameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Naive Bayes) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "#model_knn_kfold = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_nb_2 = GaussianNB()\n",
    "y_pred_kfold_nb = cross_val_predict(model_nb_2, df_X, df_Y, cv=k_fold)\n",
    "scores = cross_val_score(model_nb_2, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_nb)\n",
    "conf_matrix_nb_kfold = confusion_matrix(df_Y, y_pred_kfold_nb)\n",
    "print(\"Confusion Matrix for Dt using k-fold (leave one out)\")\n",
    "print(conf_matrix_nb_kfold)\n",
    "\n",
    "\n",
    "accuracy_nb_2 = ((conf_matrix_nb_kfold[0,0] + conf_matrix_nb_kfold[1,1])/(conf_matrix_nb_kfold[0,0] +conf_matrix_nb_kfold[0,1]+conf_matrix_nb_kfold[1,0]+conf_matrix_nb_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_nb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_nb_list = []\n",
    "k_nb_specificity = []\n",
    "k_nb_sensitivity = []\n",
    "k_nb_precision = []\n",
    "k_nb_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        \n",
    "        model_nb_new = GaussianNB()\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_nb_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_nb_kfold[0][0]\n",
    "        FP = conf_matrix_nb_kfold[0][1]\n",
    "        FN = conf_matrix_nb_kfold[1][0]\n",
    "        TP = conf_matrix_nb_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_nb_list.append(row)\n",
    "    k_nb_specificity.append(row_specificity)\n",
    "    k_nb_sensitivity.append(row_sensitivity)\n",
    "    k_nb_precision.append(row_precision)\n",
    "    k_nb_f1.append(row_f1)\n",
    "    \n",
    "k_nb_list = pd.DataFrame(k_nb_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_nb_list)    \n",
    "\n",
    "k_nb_specificity = pd.DataFrame(k_nb_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_nb_specificity)\n",
    "\n",
    "k_nb_sensitivity = pd.DataFrame(k_nb_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_nb_sensitivity)\n",
    "\n",
    "k_nb_precision = pd.DataFrame(k_nb_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_nb_precision)\n",
    "\n",
    "k_nb_f1 = pd.DataFrame(k_nb_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_nb_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Logistic Regression Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "\n",
    "\n",
    "model_lr = LogisticRegression(random_state=0)\n",
    "model_lr = model_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "accuracy_lr = ((conf_matrix_lr[0,0] + conf_matrix_lr[1,1])/(conf_matrix_lr[0,0] +conf_matrix_lr[0,1]+conf_matrix_lr[1,0]+conf_matrix_lr[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_lr)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_lr))\n",
    "\n",
    "print(y_pred_lr)\n",
    "\n",
    "print(conf_matrix_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "#Create new LR object\n",
    "model_lr2 = LogisticRegression()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(model_lr2, hyperparameters, cv=cv)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (LR)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(C= 10, penalty='l2',solver= 'newton-cg')\n",
    "model_lr = model_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_lr_2 = confusion_matrix(y_test, y_pred_lr_2)\n",
    "\n",
    "accuracy_lr_2 = ((conf_matrix_lr_2[0,0] + conf_matrix_lr_2[1,1])/(conf_matrix_lr_2[0,0] +conf_matrix_lr_2[0,1]+conf_matrix_lr_2[1,0]+conf_matrix_lr_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_lr_2)\n",
    "print(accuracy_lr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (LR) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_lr_list = []\n",
    "k_lr_specificity = []\n",
    "k_lr_sensitivity = []\n",
    "k_lr_precision = []\n",
    "k_lr_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        model_nb_new = LogisticRegression(C= 10, penalty='l2',solver= 'newton-cg')\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_lr_list.append(row)\n",
    "    k_lr_specificity.append(row_specificity)\n",
    "    k_lr_sensitivity.append(row_sensitivity)\n",
    "    k_lr_precision.append(row_precision)\n",
    "    k_lr_f1.append(row_f1)\n",
    "    \n",
    "k_lr_list = pd.DataFrame(k_lr_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_lr_list)    \n",
    "\n",
    "k_lr_specificity = pd.DataFrame(k_lr_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_lr_specificity)\n",
    "\n",
    "k_lr_sensitivity = pd.DataFrame(k_lr_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_lr_sensitivity)\n",
    "\n",
    "k_lr_precision = pd.DataFrame(k_lr_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_lr_precision)\n",
    "\n",
    "k_lr_f1 = pd.DataFrame(k_lr_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Gradient Boosting Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "\n",
    "model_gb = GradientBoostingClassifier(random_state=0)\n",
    "model_gb = model_gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = model_gb.predict(X_test)\n",
    "\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "accuracy_gb = ((conf_matrix_gb[0,0] + conf_matrix_gb[1,1])/(conf_matrix_gb[0,0] +conf_matrix_gb[0,1]+conf_matrix_gb[1,0]+conf_matrix_gb[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_gb)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_gb))\n",
    "\n",
    "print(y_pred_gb)\n",
    "\n",
    "print(conf_matrix_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200] #[5,50,250,500]\n",
    "max_depth = [1,3,5,7,9]\n",
    "learning_rate = [1, 0.5, 0.25, 0.1, 0.05, 0.01] #[0.01,0.1,1,10,100] \n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(n_estimators=n_estimators,max_depth=max_depth,learning_rate=learning_rate)\n",
    "\n",
    "#Create new LR object\n",
    "model_gb2 = GradientBoostingClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(model_gb2, hyperparameters, cv=10)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (GB)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_gb = GradientBoostingClassifier(learning_rate= 0.5, max_depth=1,n_estimators=3)\n",
    "model_gb = model_gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb_2 = model_gb.predict(X_test)\n",
    "\n",
    "conf_matrix_gb_2 = confusion_matrix(y_test, y_pred_gb_2)\n",
    "\n",
    "accuracy_gb_2 = ((conf_matrix_gb_2[0,0] + conf_matrix_gb_2[1,1])/(conf_matrix_gb_2[0,0] +conf_matrix_gb_2[0,1]+conf_matrix_gb_2[1,0]+conf_matrix_gb_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_gb_2)\n",
    "print(accuracy_gb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (GB) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_gb_list = []\n",
    "k_gb_specificity = []\n",
    "k_gb_sensitivity = []\n",
    "k_gb_precision = []\n",
    "k_gb_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling        \n",
    "        model_nb_new = GradientBoostingClassifier(learning_rate= 0.5, max_depth=1,n_estimators=3)\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_gb_list.append(row)\n",
    "    k_gb_specificity.append(row_specificity)\n",
    "    k_gb_sensitivity.append(row_sensitivity)\n",
    "    k_gb_precision.append(row_precision)\n",
    "    k_gb_f1.append(row_f1)\n",
    "    \n",
    "k_gb_list = pd.DataFrame(k_gb_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_gb_list)    \n",
    "\n",
    "k_gb_specificity = pd.DataFrame(k_gb_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_gb_specificity)\n",
    "\n",
    "k_gb_sensitivity = pd.DataFrame(k_gb_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_gb_sensitivity)\n",
    "\n",
    "k_gb_precision = pd.DataFrame(k_gb_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_gb_precision)\n",
    "\n",
    "k_gb_f1 = pd.DataFrame(k_gb_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_gb_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Random Forest Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "\n",
    "model_gb = RandomForestClassifier()\n",
    "model_gb = model_gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = model_gb.predict(X_test)\n",
    "\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "accuracy_gb = ((conf_matrix_gb[0,0] + conf_matrix_gb[1,1])/(conf_matrix_gb[0,0] +conf_matrix_gb[0,1]+conf_matrix_gb[1,0]+conf_matrix_gb[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_gb)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_gb))\n",
    "\n",
    "print(y_pred_gb)\n",
    "\n",
    "print(conf_matrix_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200] #[5,50,250,500]\n",
    "max_depth = [1,3,5,7,9]\n",
    "max_features = ['auto', 'sqrt', 'log2'] #[0.01,0.1,1,10,100] \n",
    "min_samples_split = [2,5,10]\n",
    "min_samples_leaf = [1,2,5,10,15]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(n_estimators=n_estimators,max_depth=max_depth,max_features=max_features, min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf )\n",
    "\n",
    "#Create new LR object\n",
    "model_gb2 = RandomForestClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(model_gb2, hyperparameters, cv=10)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (Random Forest)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_gb = RandomForestClassifier(n_estimators=16,max_depth=3,max_features='auto', min_samples_split=10,min_samples_leaf=5)\n",
    "model_gb = model_gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb_2 = model_gb.predict(X_test)\n",
    "\n",
    "conf_matrix_gb_2 = confusion_matrix(y_test, y_pred_gb_2)\n",
    "\n",
    "accuracy_gb_2 = ((conf_matrix_gb_2[0,0] + conf_matrix_gb_2[1,1])/(conf_matrix_gb_2[0,0] +conf_matrix_gb_2[0,1]+conf_matrix_gb_2[1,0]+conf_matrix_gb_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_gb_2)\n",
    "print(accuracy_gb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (RF) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "print(kfold)\n",
    "\n",
    "k_rf_list = []\n",
    "k_rf_specificity = []\n",
    "k_rf_sensitivity = []\n",
    "k_rf_precision = []\n",
    "k_rf_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        print('\\ntrain: %s, test: %s' % (train.size, test.size))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling              \n",
    "        model_nb_new = RandomForestClassifier(n_estimators=16,max_depth=3,max_features='auto', min_samples_split=10,min_samples_leaf=5)\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_rf_list.append(row)\n",
    "    k_rf_specificity.append(row_specificity)\n",
    "    k_rf_sensitivity.append(row_sensitivity)\n",
    "    k_rf_precision.append(row_precision)\n",
    "    k_rf_f1.append(row_f1)\n",
    "    \n",
    "k_rf_list = pd.DataFrame(k_rf_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_rf_list)    \n",
    "\n",
    "k_rf_specificity = pd.DataFrame(k_rf_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_rf_specificity)\n",
    "\n",
    "k_rf_sensitivity = pd.DataFrame(k_rf_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_rf_sensitivity)\n",
    "\n",
    "k_rf_precision = pd.DataFrame(k_rf_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_rf_precision)\n",
    "\n",
    "k_rf_f1 = pd.DataFrame(k_rf_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_rf_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = []\n",
    "\n",
    "##################classifictaion accuracy\n",
    "knn_mean_accuracy = k_list[\"mean accuracy\"].mean()\n",
    "dt_mean_accuracy = k_dt_list[\"mean accuracy\"].mean()\n",
    "svm_mean_accuracy = k_svm_list[\"mean accuracy\"].mean()\n",
    "nb_mean_accuracy = k_nb_list[\"mean accuracy\"].mean()\n",
    "lr_mean_accuracy = k_lr_list[\"mean accuracy\"].mean()\n",
    "gb_mean_accuracy = k_gb_list[\"mean accuracy\"].mean()\n",
    "rf_mean_accuracy = k_rf_list[\"mean accuracy\"].mean()\n",
    "\n",
    "print(knn_mean_accuracy)\n",
    "print(dt_mean_accuracy)\n",
    "print(svm_mean_accuracy)\n",
    "print(nb_mean_accuracy)\n",
    "print(lr_mean_accuracy)\n",
    "print(gb_mean_accuracy)\n",
    "print(rf_mean_accuracy)\n",
    "rf_mean_accuracy\n",
    "\n",
    "classification_accuracy = ['Classification Accuracy',knn_mean_accuracy,dt_mean_accuracy,svm_mean_accuracy,nb_mean_accuracy,lr_mean_accuracy,gb_mean_accuracy,rf_mean_accuracy]\n",
    "df_comparison.append(classification_accuracy)\n",
    "\n",
    "#########################specificity\n",
    "knn_mean_specificity = k_specificity[\"mean specificity\"].mean()\n",
    "dt_mean_specificity = k_dt_specificity[\"mean specificity\"].mean()\n",
    "svm_mean_specificity = k_svm_specificity[\"mean specificity\"].mean()\n",
    "nb_mean_specificity = k_nb_specificity[\"mean specificity\"].mean()\n",
    "lr_mean_specificity = k_lr_specificity[\"mean specificity\"].mean()\n",
    "gb_mean_specificity = k_gb_specificity[\"mean specificity\"].mean()\n",
    "rf_mean_specificity = k_rf_specificity[\"mean specificity\"].mean()\n",
    "\n",
    "specificity = ['Specificity',knn_mean_specificity,dt_mean_specificity,svm_mean_specificity,nb_mean_specificity,lr_mean_specificity,gb_mean_specificity,rf_mean_specificity]\n",
    "df_comparison.append(specificity)\n",
    "\n",
    "#########################sensitivity/recall\n",
    "knn_mean_sensitivity = k_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "dt_mean_sensitivity = k_dt_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "svm_mean_sensitivity = k_svm_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "nb_mean_sensitivity = k_nb_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "lr_mean_sensitivity = k_lr_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "gb_mean_sensitivity = k_gb_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "rf_mean_sensitivity = k_rf_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "\n",
    "sensitivity = ['Sensitivity',knn_mean_sensitivity,dt_mean_sensitivity,svm_mean_sensitivity,nb_mean_sensitivity,lr_mean_sensitivity,gb_mean_sensitivity,rf_mean_sensitivity]\n",
    "df_comparison.append(sensitivity)\n",
    "\n",
    "#########################precision\n",
    "knn_mean_precision = k_precision[\"mean precision\"].mean()\n",
    "dt_mean_precision = k_dt_precision[\"mean precision\"].mean()\n",
    "svm_mean_precision = k_svm_precision[\"mean precision\"].mean()\n",
    "nb_mean_precision = k_nb_precision[\"mean precision\"].mean()\n",
    "lr_mean_precision = k_lr_precision[\"mean precision\"].mean()\n",
    "gb_mean_precision = k_gb_precision[\"mean precision\"].mean()\n",
    "rf_mean_precision = k_rf_precision[\"mean precision\"].mean()\n",
    "\n",
    "precision = ['Precision',knn_mean_precision,dt_mean_precision,svm_mean_precision,nb_mean_precision,lr_mean_precision,gb_mean_precision,rf_mean_precision]\n",
    "df_comparison.append(precision)\n",
    "\n",
    "#########################F1 score\n",
    "knn_mean_f1 = k_f1[\"mean f1 score\"].mean()/100\n",
    "dt_mean_f1 = k_dt_f1[\"mean f1 score\"].mean()/100\n",
    "svm_mean_f1 = k_svm_f1[\"mean f1 score\"].mean()/100\n",
    "nb_mean_f1 = k_nb_f1[\"mean f1 score\"].mean()/100\n",
    "lr_mean_f1 = k_lr_f1[\"mean f1 score\"].mean()/100\n",
    "gb_mean_f1 = k_gb_f1[\"mean f1 score\"].mean()/100\n",
    "rf_mean_f1 = k_rf_f1[\"mean f1 score\"].mean()/100\n",
    "\n",
    "f1 = ['f1 score',knn_mean_f1,dt_mean_f1,svm_mean_f1,nb_mean_f1,lr_mean_f1,gb_mean_f1,rf_mean_f1]\n",
    "df_comparison.append(f1)\n",
    "\n",
    "df_comparison = pd.DataFrame(df_comparison, columns=[\"Performance Metrics\", \"KNN\", \"Decision Trees\",\"SVM\", \"Naive Bayes\", \"Logistic Regression\", \"GB\", \"Random Forest\"])\n",
    "df_comparison = df_comparison.set_index('Performance Metrics')\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df_comparison.loc[ ['Classification Accuracy' , 'Specificity Accuracy'] , : ]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison.plot(kind=\"bar\", figsize=(15, 10))\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel(\"Average performance values\")\n",
    "plt.title(\"Average values of each performance metric using MDVR-KCL Acoustic Features only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison[['KNN','Decision Trees']].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
