{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on MDVR-KCL Dataset (in chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import auc\n",
    "#from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Acoustic Features alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data that was extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanF0Hz</th>\n",
       "      <th>stdevF0Hz</th>\n",
       "      <th>HNR</th>\n",
       "      <th>localJitter</th>\n",
       "      <th>localabsoluteJitter</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>apq5Shimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>231.250953</td>\n",
       "      <td>45.480073</td>\n",
       "      <td>16.525302</td>\n",
       "      <td>0.027821</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>0.011947</td>\n",
       "      <td>0.085180</td>\n",
       "      <td>0.922601</td>\n",
       "      <td>0.028993</td>\n",
       "      <td>0.049585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201.477299</td>\n",
       "      <td>14.236377</td>\n",
       "      <td>18.961886</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.075751</td>\n",
       "      <td>0.843053</td>\n",
       "      <td>0.021091</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209.285317</td>\n",
       "      <td>29.051820</td>\n",
       "      <td>12.840174</td>\n",
       "      <td>0.029609</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.013380</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.078756</td>\n",
       "      <td>0.813759</td>\n",
       "      <td>0.028769</td>\n",
       "      <td>0.037729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221.819604</td>\n",
       "      <td>31.556270</td>\n",
       "      <td>16.250999</td>\n",
       "      <td>0.027461</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.088893</td>\n",
       "      <td>0.941172</td>\n",
       "      <td>0.033405</td>\n",
       "      <td>0.054926</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199.675876</td>\n",
       "      <td>6.519508</td>\n",
       "      <td>14.700531</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.014263</td>\n",
       "      <td>0.016973</td>\n",
       "      <td>0.078197</td>\n",
       "      <td>0.684674</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.049475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     meanF0Hz  stdevF0Hz        HNR  localJitter  localabsoluteJitter  \\\n",
       "0  231.250953  45.480073  16.525302     0.027821             0.000121   \n",
       "1  201.477299  14.236377  18.961886     0.014327             0.000071   \n",
       "2  209.285317  29.051820  12.840174     0.029609             0.000143   \n",
       "3  221.819604  31.556270  16.250999     0.027461             0.000123   \n",
       "4  199.675876   6.519508  14.700531     0.026505             0.000134   \n",
       "\n",
       "   rapJitter  ppq5Jitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "0   0.011726    0.011947      0.085180        0.922601     0.028993   \n",
       "1   0.006781    0.007266      0.075751        0.843053     0.021091   \n",
       "2   0.013380    0.009806      0.078756        0.813759     0.028769   \n",
       "3   0.012908    0.015317      0.088893        0.941172     0.033405   \n",
       "4   0.014263    0.016973      0.078197        0.684674     0.030303   \n",
       "\n",
       "   apq5Shimmer  label  \n",
       "0     0.049585      0  \n",
       "1     0.034795      0  \n",
       "2     0.037729      0  \n",
       "3     0.054926      0  \n",
       "4     0.049475      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only acoustic features\n",
    "df = pd.read_csv(\"../MDVR_acoustic_features_chunks.csv\")\n",
    "#df = shuffle(df)\n",
    "#df.reset_index(inplace=True, drop=True)\n",
    "df.drop('voiceID', inplace = True, axis = 1)\n",
    "df['label'].value_counts()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816, 12)\n",
      "         meanF0Hz   stdevF0Hz         HNR  localJitter  localabsoluteJitter  \\\n",
      "count  812.000000  811.000000  816.000000   812.000000           812.000000   \n",
      "mean   180.559539   25.165668   13.095275     0.022565             0.000134   \n",
      "std     49.148873   17.934206    3.216097     0.012113             0.000075   \n",
      "min     92.441819    1.031706    2.928637     0.003969             0.000020   \n",
      "25%    143.652052   12.161171   10.916644     0.016490             0.000084   \n",
      "50%    182.401213   20.701936   12.878943     0.020791             0.000118   \n",
      "75%    207.356938   33.586008   15.167071     0.026110             0.000164   \n",
      "max    461.023006  143.864146   25.986014     0.124222             0.000672   \n",
      "\n",
      "        rapJitter  ppq5Jitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
      "count  811.000000  810.000000    811.000000      811.000000   811.000000   \n",
      "mean     0.009860    0.011048      0.091210        0.894692     0.033915   \n",
      "std      0.006451    0.008284      0.031618        0.253281     0.016585   \n",
      "min      0.001375    0.001447      0.030044        0.295598     0.010385   \n",
      "25%      0.006817    0.007520      0.071749        0.731535     0.024726   \n",
      "50%      0.008877    0.009885      0.087039        0.880400     0.030750   \n",
      "75%      0.011495    0.012409      0.104099        1.019691     0.039205   \n",
      "max      0.074209    0.139163      0.392267        2.377180     0.219708   \n",
      "\n",
      "       apq5Shimmer       label  \n",
      "count   808.000000  816.000000  \n",
      "mean      0.048662    0.419118  \n",
      "std       0.021141    0.493717  \n",
      "min       0.013275    0.000000  \n",
      "25%       0.035949    0.000000  \n",
      "50%       0.044928    0.000000  \n",
      "75%       0.056801    1.000000  \n",
      "max       0.268241    1.000000  \n",
      "label\n",
      "0    474\n",
      "1    342\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# shape\n",
    "print(df.shape)\n",
    "\n",
    "# descriptions\n",
    "print(df.describe())\n",
    "\n",
    "# class distribution\n",
    "print(df.groupby('label').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Check outliers Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histograms\n",
    "plt.figure(figsize=(30,30))\n",
    "#df_all.hist(figsize=(30,30))\n",
    "#plt.show()\n",
    "\n",
    "#box plot to check outliers\n",
    "cols = df.columns.drop('label')\n",
    "fig, ax = plt.subplots(4,3, figsize=(30,30))\n",
    "for i,t in enumerate(cols):\n",
    "    sns.boxplot(y=t, x= \"label\", data=df, ax=ax[i//3,i % 3])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.39020739220449e-05\n",
      "0.000163782224326925\n",
      "7.98801504048801e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanF0Hz</th>\n",
       "      <th>stdevF0Hz</th>\n",
       "      <th>HNR</th>\n",
       "      <th>localJitter</th>\n",
       "      <th>localabsoluteJitter</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>apq5Shimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>105.624426</td>\n",
       "      <td>51.261106</td>\n",
       "      <td>14.181993</td>\n",
       "      <td>0.029655</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.015782</td>\n",
       "      <td>0.076714</td>\n",
       "      <td>0.849089</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>0.038751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>118.070067</td>\n",
       "      <td>9.636819</td>\n",
       "      <td>10.699646</td>\n",
       "      <td>0.033656</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>0.116175</td>\n",
       "      <td>1.141910</td>\n",
       "      <td>0.042764</td>\n",
       "      <td>0.058444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>122.925231</td>\n",
       "      <td>8.232303</td>\n",
       "      <td>14.867126</td>\n",
       "      <td>0.035302</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.018468</td>\n",
       "      <td>0.017671</td>\n",
       "      <td>0.082776</td>\n",
       "      <td>0.824150</td>\n",
       "      <td>0.028361</td>\n",
       "      <td>0.039233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>349.748554</td>\n",
       "      <td>85.982228</td>\n",
       "      <td>17.152853</td>\n",
       "      <td>0.092147</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.023419</td>\n",
       "      <td>0.131716</td>\n",
       "      <td>1.091028</td>\n",
       "      <td>0.054938</td>\n",
       "      <td>0.066203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>102.620464</td>\n",
       "      <td>13.859461</td>\n",
       "      <td>14.239776</td>\n",
       "      <td>0.029050</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.058137</td>\n",
       "      <td>0.567546</td>\n",
       "      <td>0.017158</td>\n",
       "      <td>0.024261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>105.173298</td>\n",
       "      <td>22.991949</td>\n",
       "      <td>14.099778</td>\n",
       "      <td>0.030932</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.013938</td>\n",
       "      <td>0.075083</td>\n",
       "      <td>0.740534</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.036424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>93.014171</td>\n",
       "      <td>9.442112</td>\n",
       "      <td>13.019803</td>\n",
       "      <td>0.027584</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.007696</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>0.074712</td>\n",
       "      <td>0.749408</td>\n",
       "      <td>0.025947</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>99.176686</td>\n",
       "      <td>17.190880</td>\n",
       "      <td>12.637254</td>\n",
       "      <td>0.030614</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.009385</td>\n",
       "      <td>0.015430</td>\n",
       "      <td>0.064547</td>\n",
       "      <td>0.654457</td>\n",
       "      <td>0.022477</td>\n",
       "      <td>0.030985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>110.706257</td>\n",
       "      <td>10.102389</td>\n",
       "      <td>9.890545</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.012094</td>\n",
       "      <td>0.014893</td>\n",
       "      <td>0.108543</td>\n",
       "      <td>1.029208</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>107.479975</td>\n",
       "      <td>11.002565</td>\n",
       "      <td>11.454369</td>\n",
       "      <td>0.033393</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.013343</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>0.127687</td>\n",
       "      <td>1.152064</td>\n",
       "      <td>0.049104</td>\n",
       "      <td>0.069177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>123.795859</td>\n",
       "      <td>4.038564</td>\n",
       "      <td>9.140992</td>\n",
       "      <td>0.038153</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.024121</td>\n",
       "      <td>0.031045</td>\n",
       "      <td>0.281851</td>\n",
       "      <td>2.002528</td>\n",
       "      <td>0.168703</td>\n",
       "      <td>0.268241</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>108.159900</td>\n",
       "      <td>9.921206</td>\n",
       "      <td>12.502560</td>\n",
       "      <td>0.034110</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.019009</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.965892</td>\n",
       "      <td>0.031810</td>\n",
       "      <td>0.056074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>106.789684</td>\n",
       "      <td>12.045635</td>\n",
       "      <td>9.164555</td>\n",
       "      <td>0.034269</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.014072</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>0.121578</td>\n",
       "      <td>1.197362</td>\n",
       "      <td>0.047388</td>\n",
       "      <td>0.058934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>117.240612</td>\n",
       "      <td>10.415259</td>\n",
       "      <td>6.506905</td>\n",
       "      <td>0.037748</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.017169</td>\n",
       "      <td>0.016130</td>\n",
       "      <td>0.129982</td>\n",
       "      <td>1.229469</td>\n",
       "      <td>0.066557</td>\n",
       "      <td>0.099828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>92.441819</td>\n",
       "      <td>8.923055</td>\n",
       "      <td>11.600720</td>\n",
       "      <td>0.030363</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.077221</td>\n",
       "      <td>0.760205</td>\n",
       "      <td>0.025019</td>\n",
       "      <td>0.038069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>114.727650</td>\n",
       "      <td>12.476625</td>\n",
       "      <td>11.093573</td>\n",
       "      <td>0.038840</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.019539</td>\n",
       "      <td>0.015149</td>\n",
       "      <td>0.108776</td>\n",
       "      <td>1.077431</td>\n",
       "      <td>0.048363</td>\n",
       "      <td>0.048369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>216.489336</td>\n",
       "      <td>18.044344</td>\n",
       "      <td>9.818812</td>\n",
       "      <td>0.072433</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.042978</td>\n",
       "      <td>0.203619</td>\n",
       "      <td>1.834492</td>\n",
       "      <td>0.182733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>105.900786</td>\n",
       "      <td>6.950439</td>\n",
       "      <td>10.009566</td>\n",
       "      <td>0.035813</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.016435</td>\n",
       "      <td>0.123347</td>\n",
       "      <td>1.083617</td>\n",
       "      <td>0.045549</td>\n",
       "      <td>0.069448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>113.366607</td>\n",
       "      <td>13.061917</td>\n",
       "      <td>9.638827</td>\n",
       "      <td>0.039851</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.018446</td>\n",
       "      <td>0.019510</td>\n",
       "      <td>0.097406</td>\n",
       "      <td>0.925732</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>0.062130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>108.114987</td>\n",
       "      <td>4.067098</td>\n",
       "      <td>9.975785</td>\n",
       "      <td>0.038658</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.023709</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>0.127052</td>\n",
       "      <td>1.213692</td>\n",
       "      <td>0.048779</td>\n",
       "      <td>0.059405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>158.764324</td>\n",
       "      <td>28.761099</td>\n",
       "      <td>5.967175</td>\n",
       "      <td>0.056439</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>0.008491</td>\n",
       "      <td>0.147794</td>\n",
       "      <td>1.221955</td>\n",
       "      <td>0.076565</td>\n",
       "      <td>0.105992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>115.579189</td>\n",
       "      <td>66.926741</td>\n",
       "      <td>11.570552</td>\n",
       "      <td>0.040818</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.019452</td>\n",
       "      <td>0.022387</td>\n",
       "      <td>0.110820</td>\n",
       "      <td>1.105300</td>\n",
       "      <td>0.046892</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>94.704772</td>\n",
       "      <td>18.770048</td>\n",
       "      <td>11.292347</td>\n",
       "      <td>0.035065</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>0.017610</td>\n",
       "      <td>0.084445</td>\n",
       "      <td>0.828761</td>\n",
       "      <td>0.024832</td>\n",
       "      <td>0.041062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>102.408318</td>\n",
       "      <td>9.718174</td>\n",
       "      <td>10.665477</td>\n",
       "      <td>0.038931</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.016282</td>\n",
       "      <td>0.026134</td>\n",
       "      <td>0.129739</td>\n",
       "      <td>1.237910</td>\n",
       "      <td>0.049775</td>\n",
       "      <td>0.074508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>105.058732</td>\n",
       "      <td>10.513483</td>\n",
       "      <td>5.497981</td>\n",
       "      <td>0.040219</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>0.012351</td>\n",
       "      <td>0.149535</td>\n",
       "      <td>1.291920</td>\n",
       "      <td>0.043209</td>\n",
       "      <td>0.052296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>109.695065</td>\n",
       "      <td>9.657805</td>\n",
       "      <td>7.741675</td>\n",
       "      <td>0.045696</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.021559</td>\n",
       "      <td>0.160831</td>\n",
       "      <td>1.433544</td>\n",
       "      <td>0.053934</td>\n",
       "      <td>0.102092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>97.565199</td>\n",
       "      <td>9.385452</td>\n",
       "      <td>11.190016</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.016316</td>\n",
       "      <td>0.019654</td>\n",
       "      <td>0.110194</td>\n",
       "      <td>0.987060</td>\n",
       "      <td>0.034662</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>101.547701</td>\n",
       "      <td>10.209203</td>\n",
       "      <td>8.811301</td>\n",
       "      <td>0.046944</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.021961</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>0.138575</td>\n",
       "      <td>1.294185</td>\n",
       "      <td>0.061351</td>\n",
       "      <td>0.085327</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>94.540019</td>\n",
       "      <td>2.352628</td>\n",
       "      <td>5.628797</td>\n",
       "      <td>0.048927</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.029446</td>\n",
       "      <td>0.021755</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>1.542377</td>\n",
       "      <td>0.093695</td>\n",
       "      <td>0.090756</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>125.587205</td>\n",
       "      <td>8.962313</td>\n",
       "      <td>12.439040</td>\n",
       "      <td>0.067079</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.038959</td>\n",
       "      <td>0.034317</td>\n",
       "      <td>0.147005</td>\n",
       "      <td>1.155790</td>\n",
       "      <td>0.039582</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>217.784077</td>\n",
       "      <td>37.076851</td>\n",
       "      <td>11.670683</td>\n",
       "      <td>0.115380</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.068436</td>\n",
       "      <td>0.089811</td>\n",
       "      <td>0.202285</td>\n",
       "      <td>1.826999</td>\n",
       "      <td>0.103223</td>\n",
       "      <td>0.155980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>182.826527</td>\n",
       "      <td>31.102973</td>\n",
       "      <td>14.012335</td>\n",
       "      <td>0.122154</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.045864</td>\n",
       "      <td>0.139163</td>\n",
       "      <td>0.127022</td>\n",
       "      <td>1.100419</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.077514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>105.610168</td>\n",
       "      <td>9.351154</td>\n",
       "      <td>4.920061</td>\n",
       "      <td>0.071081</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.040999</td>\n",
       "      <td>0.026774</td>\n",
       "      <td>0.149796</td>\n",
       "      <td>1.438537</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.053734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       meanF0Hz  stdevF0Hz        HNR  localJitter  localabsoluteJitter  \\\n",
       "695  105.624426  51.261106  14.181993     0.029655             0.000285   \n",
       "539  118.070067   9.636819  10.699646     0.033656             0.000286   \n",
       "704  122.925231   8.232303  14.867126     0.035302             0.000287   \n",
       "186  349.748554  85.982228  17.152853     0.092147             0.000288   \n",
       "692  102.620464  13.859461  14.239776     0.029050             0.000288   \n",
       "689  105.173298  22.991949  14.099778     0.030932             0.000296   \n",
       "696   93.014171   9.442112  13.019803     0.027584             0.000297   \n",
       "697   99.176686  17.190880  12.637254     0.030614             0.000309   \n",
       "515  110.706257  10.102389   9.890545     0.034401             0.000311   \n",
       "506  107.479975  11.002565  11.454369     0.033393             0.000311   \n",
       "446  123.795859   4.038564   9.140992     0.038153             0.000312   \n",
       "517  108.159900   9.921206  12.502560     0.034110             0.000317   \n",
       "526  106.789684  12.045635   9.164555     0.034269             0.000320   \n",
       "550  117.240612  10.415259   6.506905     0.037748             0.000322   \n",
       "690   92.441819   8.923055  11.600720     0.030363             0.000331   \n",
       "543  114.727650  12.476625  11.093573     0.038840             0.000335   \n",
       "749  216.489336  18.044344   9.818812     0.072433             0.000336   \n",
       "518  105.900786   6.950439  10.009566     0.035813             0.000337   \n",
       "511  113.366607  13.061917   9.638827     0.039851             0.000351   \n",
       "533  108.114987   4.067098   9.975785     0.038658             0.000357   \n",
       "443  158.764324  28.761099   5.967175     0.056439             0.000358   \n",
       "509  115.579189  66.926741  11.570552     0.040818             0.000368   \n",
       "698   94.704772  18.770048  11.292347     0.035065             0.000375   \n",
       "520  102.408318   9.718174  10.665477     0.038931             0.000383   \n",
       "521  105.058732  10.513483   5.497981     0.040219             0.000384   \n",
       "504  109.695065   9.657805   7.741675     0.045696             0.000415   \n",
       "688   97.565199   9.385452  11.190016     0.042722             0.000441   \n",
       "510  101.547701  10.209203   8.811301     0.046944             0.000462   \n",
       "524   94.540019   2.352628   5.628797     0.048927             0.000511   \n",
       "672  125.587205   8.962313  12.439040     0.067079             0.000536   \n",
       "709  217.784077  37.076851  11.670683     0.115380             0.000539   \n",
       "706  182.826527  31.102973  14.012335     0.122154             0.000669   \n",
       "525  105.610168   9.351154   4.920061     0.071081             0.000672   \n",
       "\n",
       "     rapJitter  ppq5Jitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "695   0.010280    0.015782      0.076714        0.849089     0.022786   \n",
       "539   0.014336    0.015074      0.116175        1.141910     0.042764   \n",
       "704   0.018468    0.017671      0.082776        0.824150     0.028361   \n",
       "186   0.046544    0.023419      0.131716        1.091028     0.054938   \n",
       "692   0.009480    0.009917      0.058137        0.567546     0.017158   \n",
       "689   0.010441    0.013938      0.075083        0.740534     0.021738   \n",
       "696   0.007696    0.013530      0.074712        0.749408     0.025947   \n",
       "697   0.009385    0.015430      0.064547        0.654457     0.022477   \n",
       "515   0.012094    0.014893      0.108543        1.029208     0.025679   \n",
       "506   0.013343    0.015433      0.127687        1.152064     0.049104   \n",
       "446   0.024121    0.031045      0.281851        2.002528     0.168703   \n",
       "517   0.015271    0.019009      0.096100        0.965892     0.031810   \n",
       "526   0.014072    0.014479      0.121578        1.197362     0.047388   \n",
       "550   0.017169    0.016130      0.129982        1.229469     0.066557   \n",
       "690   0.009014    0.012686      0.077221        0.760205     0.025019   \n",
       "543   0.019539    0.015149      0.108776        1.077431     0.048363   \n",
       "749   0.021526    0.042978      0.203619        1.834492     0.182733   \n",
       "518   0.012726    0.016435      0.123347        1.083617     0.045549   \n",
       "511   0.018446    0.019510      0.097406        0.925732     0.040767   \n",
       "533   0.023709    0.019469      0.127052        1.213692     0.048779   \n",
       "443   0.015832    0.008491      0.147794        1.221955     0.076565   \n",
       "509   0.019452    0.022387      0.110820        1.105300     0.046892   \n",
       "698   0.010380    0.017610      0.084445        0.828761     0.024832   \n",
       "520   0.016282    0.026134      0.129739        1.237910     0.049775   \n",
       "521   0.018856    0.012351      0.149535        1.291920     0.043209   \n",
       "504   0.019851    0.021559      0.160831        1.433544     0.053934   \n",
       "688   0.016316    0.019654      0.110194        0.987060     0.034662   \n",
       "510   0.021961    0.022543      0.138575        1.294185     0.061351   \n",
       "524   0.029446    0.021755      0.162813        1.542377     0.093695   \n",
       "672   0.038959    0.034317      0.147005        1.155790     0.039582   \n",
       "709   0.068436    0.089811      0.202285        1.826999     0.103223   \n",
       "706   0.045864    0.139163      0.127022        1.100419     0.068303   \n",
       "525   0.040999    0.026774      0.149796        1.438537     0.084746   \n",
       "\n",
       "     apq5Shimmer  label  \n",
       "695     0.038751      1  \n",
       "539     0.058444      1  \n",
       "704     0.039233      1  \n",
       "186     0.066203      0  \n",
       "692     0.024261      1  \n",
       "689     0.036424      1  \n",
       "696     0.041783      1  \n",
       "697     0.030985      1  \n",
       "515     0.051402      1  \n",
       "506     0.069177      1  \n",
       "446     0.268241      0  \n",
       "517     0.056074      1  \n",
       "526     0.058934      1  \n",
       "550     0.099828      1  \n",
       "690     0.038069      1  \n",
       "543     0.048369      1  \n",
       "749          NaN      1  \n",
       "518     0.069448      1  \n",
       "511     0.062130      1  \n",
       "533     0.059405      1  \n",
       "443     0.105992      0  \n",
       "509     0.065678      1  \n",
       "698     0.041062      1  \n",
       "520     0.074508      1  \n",
       "521     0.052296      1  \n",
       "504     0.102092      1  \n",
       "688     0.057010      1  \n",
       "510     0.085327      1  \n",
       "524     0.090756      1  \n",
       "672     0.067725      1  \n",
       "709     0.155980      1  \n",
       "706     0.077514      1  \n",
       "525     0.053734      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jitter = df.sort_values(\"localabsoluteJitter\")\n",
    "Q1=df_jitter['localabsoluteJitter'].quantile(0.25)\n",
    "Q3=df_jitter['localabsoluteJitter'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "Lower_Whisker = Q1 - 1.5*IQR\n",
    "Upper_Whisker = Q3 + 1.5*IQR\n",
    "\n",
    "df_1 = df_jitter[df_jitter['localabsoluteJitter'] > Upper_Whisker]\n",
    "df_1.append(df_jitter[df_jitter['localabsoluteJitter'] < Lower_Whisker])\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#separate dependent and independent variable for acoustic features only\n",
    "X = df.iloc[:, :-1]\n",
    "df_X = df.iloc[:, :-1].values\n",
    "df_Y = df.iloc[:,-1].values\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.237427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823554</td>\n",
       "      <td>0.283364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.094979</td>\n",
       "      <td>0.114115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.252482</td>\n",
       "      <td>0.064313</td>\n",
       "      <td>0.591611</td>\n",
       "      <td>0.064528</td>\n",
       "      <td>0.065250</td>\n",
       "      <td>0.051299</td>\n",
       "      <td>0.027930</td>\n",
       "      <td>0.069236</td>\n",
       "      <td>0.108755</td>\n",
       "      <td>0.043142</td>\n",
       "      <td>0.059042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.521998</td>\n",
       "      <td>0.582137</td>\n",
       "      <td>0.118796</td>\n",
       "      <td>0.377051</td>\n",
       "      <td>0.234815</td>\n",
       "      <td>0.394274</td>\n",
       "      <td>0.197918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.278030</td>\n",
       "      <td>0.183177</td>\n",
       "      <td>0.326492</td>\n",
       "      <td>0.086266</td>\n",
       "      <td>0.081258</td>\n",
       "      <td>0.059505</td>\n",
       "      <td>0.030342</td>\n",
       "      <td>0.125338</td>\n",
       "      <td>0.179865</td>\n",
       "      <td>0.076182</td>\n",
       "      <td>0.091074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.115158</td>\n",
       "      <td>0.073457</td>\n",
       "      <td>0.565026</td>\n",
       "      <td>0.114732</td>\n",
       "      <td>0.170497</td>\n",
       "      <td>0.066223</td>\n",
       "      <td>0.049870</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.225547</td>\n",
       "      <td>0.093413</td>\n",
       "      <td>0.125171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.154805</td>\n",
       "      <td>0.491574</td>\n",
       "      <td>0.161884</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>0.113071</td>\n",
       "      <td>0.049829</td>\n",
       "      <td>0.240530</td>\n",
       "      <td>0.391958</td>\n",
       "      <td>0.235921</td>\n",
       "      <td>0.219085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.112433</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.358028</td>\n",
       "      <td>0.148040</td>\n",
       "      <td>0.217738</td>\n",
       "      <td>0.117707</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.142734</td>\n",
       "      <td>0.244324</td>\n",
       "      <td>0.136288</td>\n",
       "      <td>0.161371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.060235</td>\n",
       "      <td>0.618709</td>\n",
       "      <td>0.097651</td>\n",
       "      <td>0.090587</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.049701</td>\n",
       "      <td>0.103558</td>\n",
       "      <td>0.166285</td>\n",
       "      <td>0.098402</td>\n",
       "      <td>0.099830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>0.097778</td>\n",
       "      <td>0.087673</td>\n",
       "      <td>0.510170</td>\n",
       "      <td>0.186992</td>\n",
       "      <td>0.284730</td>\n",
       "      <td>0.140912</td>\n",
       "      <td>0.090747</td>\n",
       "      <td>0.125607</td>\n",
       "      <td>0.235168</td>\n",
       "      <td>0.077911</td>\n",
       "      <td>0.104038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>0.095262</td>\n",
       "      <td>0.064334</td>\n",
       "      <td>0.450850</td>\n",
       "      <td>0.138570</td>\n",
       "      <td>0.216686</td>\n",
       "      <td>0.102551</td>\n",
       "      <td>0.060973</td>\n",
       "      <td>0.186479</td>\n",
       "      <td>0.279149</td>\n",
       "      <td>0.127709</td>\n",
       "      <td>0.160270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>571 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    1.000000  0.237427  1.000000  0.823554  0.283364  1.000000       NaN   \n",
       "1    0.252482  0.064313  0.591611  0.064528  0.065250  0.051299  0.027930   \n",
       "2    0.521998  0.582137  0.118796  0.377051  0.234815  0.394274  0.197918   \n",
       "3    0.278030  0.183177  0.326492  0.086266  0.081258  0.059505  0.030342   \n",
       "4    0.115158  0.073457  0.565026  0.114732  0.170497  0.066223  0.049870   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "566  0.231661  0.154805  0.491574  0.161884  0.171904  0.113071  0.049829   \n",
       "567  0.112433  0.163675  0.358028  0.148040  0.217738  0.117707  0.060600   \n",
       "568  0.283998  0.060235  0.618709  0.097651  0.090587  0.069430  0.049701   \n",
       "569  0.097778  0.087673  0.510170  0.186992  0.284730  0.140912  0.090747   \n",
       "570  0.095262  0.064334  0.450850  0.138570  0.216686  0.102551  0.060973   \n",
       "\n",
       "           7         8         9         10  \n",
       "0    0.094979  0.114115  0.000000       NaN  \n",
       "1    0.069236  0.108755  0.043142  0.059042  \n",
       "2    1.000000  0.973958  1.000000  0.077812  \n",
       "3    0.125338  0.179865  0.076182  0.091074  \n",
       "4    0.154800  0.225547  0.093413  0.125171  \n",
       "..        ...       ...       ...       ...  \n",
       "566  0.240530  0.391958  0.235921  0.219085  \n",
       "567  0.142734  0.244324  0.136288  0.161371  \n",
       "568  0.103558  0.166285  0.098402  0.099830  \n",
       "569  0.125607  0.235168  0.077911  0.104038  \n",
       "570  0.186479  0.279149  0.127709  0.160270  \n",
       "\n",
       "[571 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale for acoustic features\n",
    "#sc = StandardScaler()\n",
    "sc = MinMaxScaler()\n",
    "#sc = RobustScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****************KNN Experiments******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-364ca878bc66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#KNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_knn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my_pred_knn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"requires_y\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m    957\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    112\u001b[0m         ):\n\u001b[0;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"infinity\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"NaN, infinity\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#KNN\n",
    "model_knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "accuracy_knn = ((conf_matrix_knn[0,0] + conf_matrix_knn[1,1])/(conf_matrix_knn[0,0] +conf_matrix_knn[0,1]+conf_matrix_knn[1,0]+conf_matrix_knn[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn))\n",
    "\n",
    "print(y_pred_knn)\n",
    "\n",
    "print(conf_matrix_knn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for KNN####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,20)) #neighbours must be < number of samples (22)\n",
    "p=[1,2]\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors = 9, p = 1, leaf_size = 1)\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "accuracy_knn = ((conf_matrix_knn[0,0] + conf_matrix_knn[1,1])/(conf_matrix_knn[0,0] +conf_matrix_knn[0,1]+conf_matrix_knn[1,0]+conf_matrix_knn[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn))\n",
    "\n",
    "print(y_pred_knn)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn)\n",
    "\n",
    "####################using the acoustic + MFCC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method -IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "model_knn_kfold = KNeighborsClassifier(n_neighbors = 9, p =1, leaf_size = 1)\n",
    "y_pred_kfold_knn = cross_val_predict(model_knn_kfold, df_X, df_Y, cv=k_fold)\n",
    "\n",
    "scores = cross_val_score(model_knn_kfold, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_knn)\n",
    "conf_matrix_knn_kfold = confusion_matrix(df_Y, y_pred_kfold_knn)\n",
    "print(\"Confusion Matrix for KNN using k-fold (leave one out)\")\n",
    "print(conf_matrix_knn_kfold)\n",
    "\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/(conf_matrix_knn_kfold[0,0] +conf_matrix_knn_kfold[0,1]+conf_matrix_knn_kfold[1,0]+conf_matrix_knn_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_list = []\n",
    "k_specificity = []\n",
    "k_sensitivity = []\n",
    "k_precision = []\n",
    "k_f1 = []\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "        #print(Ytrain_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        model_knn_new = KNeighborsClassifier(n_neighbors = 9, p =1, leaf_size = 1)\n",
    "        model_knn_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        y_pred_knn_new = model_knn_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_knn_new)\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "        \n",
    "        #total = (conf_matrix_knn_kfold[0,0] +conf_matrix_knn_kfold[0,1]+conf_matrix_knn_kfold[1,0]+conf_matrix_knn_kfold[1,1])\n",
    "\n",
    "        #accuracy_knn_kfold = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/total)*100\n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) * 100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "        #sensitivity_knn_kfold = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/total)*100\n",
    "\n",
    "        #print(\"Confusion Matrix:\\n \", conf_matrix_knn_kfold)\n",
    "        #print(\"Accuracy \", accuracy_knn_kfold)\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "        \n",
    "        #plot roc only for the first iteration\n",
    "        #if i == 1:\n",
    "           # metrics.plot_roc_curve(model_knn_new, Xtest_kfold, Ytest_kfold,name='ROC fold {}'.format(j),\n",
    "                     #    alpha=0.3, lw=1, ax=ax)\n",
    "        \n",
    "        \n",
    "    average = row.append(total/parts)\n",
    "    \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_list.append(row)\n",
    "    k_specificity.append(row_specificity)\n",
    "    k_sensitivity.append(row_sensitivity)\n",
    "    k_precision.append(row_precision)\n",
    "    k_f1.append(row_f1)\n",
    "    \n",
    "      \n",
    "k_list = pd.DataFrame(k_list, columns=['Iteration','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - Classification Accuracy\")\n",
    "print(k_list)\n",
    "    \n",
    "\n",
    "k_specificity = pd.DataFrame(k_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_specificity)\n",
    "\n",
    "k_sensitivity = pd.DataFrame(k_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_sensitivity)\n",
    "\n",
    "k_precision = pd.DataFrame(k_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_precision)\n",
    "\n",
    "k_f1 = pd.DataFrame(k_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_f1)\n",
    "    \n",
    "k_list\n",
    "    \n",
    "#print(df.sample(n=7))\n",
    "#print(df)\n",
    "\n",
    "#x = df.take(np.random.permutation(len(df))[:4])\n",
    "#x= df.sample(n=7)\n",
    "#print(x)\n",
    "#print(df.drop(x))\n",
    "\n",
    "#drop_indices = np.random.choice(df.index, 4, replace=False)\n",
    "#df_subset = df.drop(drop_indices)\n",
    "#print(drop_indices)\n",
    "#print(df_subset)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Decision Tree Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#Decision Tree\n",
    "import matplotlib.pyplot as plt\n",
    "model_dt = tree.DecisionTreeClassifier()\n",
    "model_dt = model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_dt.predict(X_test)\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "accuracy_knn = ((conf_matrix_knn[0,0] + conf_matrix_knn[1,1])/(conf_matrix_knn[0,0] +conf_matrix_knn[0,1]+conf_matrix_knn[1,0]+conf_matrix_knn[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn))\n",
    "\n",
    "print(y_pred_knn)\n",
    "\n",
    "print(conf_matrix_knn)\n",
    "\n",
    "plt.figure(figsize=(24,14))\n",
    "tree.plot_tree(model_dt, filled=True, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for KNN####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "max_depth = list(range(1,10))\n",
    "min_samples_split = list(range(2,10)) #neighbours must be < number of samples (22)\n",
    "min_samples_leaf = list(range(1,5))\n",
    "criterion=['gini','entropy']\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, criterion = criterion)\n",
    "#Create new KNN object\n",
    "dt_2 = tree.DecisionTreeClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(dt_2, hyperparameters, refit=True)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (Decision Tree)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model_dt = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_dt = tree.DecisionTreeClassifier(max_depth = 1, criterion='gini',min_samples_leaf=1, min_samples_split=2 )\n",
    "model_dt = model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_dt_2 = confusion_matrix(y_test, y_pred_dt_2)\n",
    "\n",
    "accuracy_dt_2 = ((conf_matrix_dt_2[0,0] + conf_matrix_dt_2[1,1])/(conf_matrix_dt_2[0,0] +conf_matrix_dt_2[0,1]+conf_matrix_dt_2[1,0]+conf_matrix_dt_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_dt_2)\n",
    "print(accuracy_dt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method - IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "#model_knn_kfold = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_dt = tree.DecisionTreeClassifier(max_depth = 1, criterion='gini',min_samples_leaf=1, min_samples_split=2 )\n",
    "y_pred_kfold_knn = cross_val_predict(model_dt, df_X, df_Y, cv=k_fold)\n",
    "scores = cross_val_score(model_dt, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_knn)\n",
    "conf_matrix_knn_kfold = confusion_matrix(df_Y, y_pred_kfold_knn)\n",
    "print(\"Confusion Matrix for Dt using k-fold (leave one out)\")\n",
    "print(conf_matrix_knn_kfold)\n",
    "\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/(conf_matrix_knn_kfold[0,0] +conf_matrix_knn_kfold[0,1]+conf_matrix_knn_kfold[1,0]+conf_matrix_knn_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_dt_list = []\n",
    "k_dt_specificity = []\n",
    "k_dt_sensitivity = []\n",
    "k_dt_precision = []\n",
    "k_dt_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        #model_knn_new = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "        model_dt_new = tree.DecisionTreeClassifier(max_depth = 1, criterion='gini',min_samples_leaf=1, min_samples_split=2 )\n",
    "        model_dt_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        y_pred_dt_new = model_dt_new.predict(Xtest_kfold)\n",
    "\n",
    "        #conf_matrix_dt_kfold = confusion_matrix(Ytest_kfold, y_pred_dt_new)\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_dt_new)\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_dt_list.append(row)\n",
    "    k_dt_specificity.append(row_specificity)\n",
    "    k_dt_sensitivity.append(row_sensitivity)\n",
    "    k_dt_precision.append(row_precision)\n",
    "    k_dt_f1.append(row_f1)\n",
    "    \n",
    "k_dt_list = pd.DataFrame(k_dt_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_dt_list)    \n",
    "\n",
    "k_dt_specificity = pd.DataFrame(k_dt_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_dt_specificity)\n",
    "\n",
    "k_dt_sensitivity = pd.DataFrame(k_dt_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_dt_sensitivity)\n",
    "\n",
    "k_dt_precision = pd.DataFrame(k_dt_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_dt_precision)\n",
    "\n",
    "k_dt_f1 = pd.DataFrame(k_dt_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_dt_f1)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************SVM Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#Decision Tree\n",
    "import matplotlib.pyplot as plt\n",
    "model_svm = svm.SVC()\n",
    "model_svm = model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = model_dt.predict(X_test)\n",
    "\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "accuracy_svm = ((conf_matrix_svm[0,0] + conf_matrix_svm[1,1])/(conf_matrix_svm[0,0] +conf_matrix_svm[0,1]+conf_matrix_svm[1,0]+conf_matrix_svm[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_svm)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_svm))\n",
    "\n",
    "print(y_pred_svm)\n",
    "\n",
    "print(conf_matrix_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "C = [0.1, 1, 10, 100, 1000]\n",
    "gamma = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "kernel = ['rbf']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C=C, gamma=gamma, kernel=kernel)\n",
    "#Create new KNN object\n",
    "svm2 = svm.SVC()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(svm2, hyperparameters, refit=True)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (SVM)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_svm = svm.SVC(C = 10, gamma=0.1,kernel='rbf')\n",
    "model_svm = model_svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_svm_2 = confusion_matrix(y_test, y_pred_svm_2)\n",
    "\n",
    "accuracy_svm_2 = ((conf_matrix_svm_2[0,0] + conf_matrix_svm_2[1,1])/(conf_matrix_svm_2[0,0] +conf_matrix_svm_2[0,1]+conf_matrix_svm_2[1,0]+conf_matrix_svm_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_svm_2)\n",
    "print(accuracy_svm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (SVM) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "#model_knn_kfold = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_svm = svm.SVC(C = 10, gamma=0.1,kernel='rbf')\n",
    "y_pred_kfold_svm = cross_val_predict(model_svm, df_X, df_Y, cv=k_fold)\n",
    "scores = cross_val_score(model_svm, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_knn)\n",
    "conf_matrix_svm_kfold = confusion_matrix(df_Y, y_pred_kfold_svm)\n",
    "print(\"Confusion Matrix for Dt using k-fold (leave one out)\")\n",
    "print(conf_matrix_svm_kfold)\n",
    "\n",
    "\n",
    "accuracy_svm_2 = ((conf_matrix_svm_kfold[0,0] + conf_matrix_svm_kfold[1,1])/(conf_matrix_svm_kfold[0,0] +conf_matrix_svm_kfold[0,1]+conf_matrix_svm_kfold[1,0]+conf_matrix_svm_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_svm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_svm_list = []\n",
    "k_svm_specificity = []\n",
    "k_svm_sensitivity = []\n",
    "k_svm_precision = []\n",
    "k_svm_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        \n",
    "        model_svm_new = svm.SVC(C = 10, gamma=0.1,kernel='rbf')\n",
    "        model_svm_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        y_pred_svm_new = model_svm_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_svm_new)\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_svm_list.append(row)\n",
    "    k_svm_specificity.append(row_specificity)\n",
    "    k_svm_sensitivity.append(row_sensitivity)\n",
    "    k_svm_precision.append(row_precision)\n",
    "    k_svm_f1.append(row_f1)\n",
    "    \n",
    "k_svm_list = pd.DataFrame(k_svm_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_svm_list)    \n",
    "\n",
    "k_svm_specificity = pd.DataFrame(k_svm_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_svm_specificity)\n",
    "\n",
    "k_svm_sensitivity = pd.DataFrame(k_svm_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_svm_sensitivity)\n",
    "\n",
    "k_svm_precision = pd.DataFrame(k_svm_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_svm_precision)\n",
    "\n",
    "k_svm_f1 = pd.DataFrame(k_svm_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_svm_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Naive Bayes Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "model_nb = GaussianNB()\n",
    "model_nb = model_nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_nb = model_nb.predict(X_test)\n",
    "\n",
    "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "accuracy_nb = ((conf_matrix_nb[0,0] + conf_matrix_nb[1,1])/(conf_matrix_nb[0,0] +conf_matrix_nb[0,1]+conf_matrix_nb[1,0]+conf_matrix_nb[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_nb)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_nb))\n",
    "\n",
    "print(y_pred_nb)\n",
    "\n",
    "print(conf_matrix_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB doesnt have important parameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Naive Bayes) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "#model_knn_kfold = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_nb_2 = GaussianNB()\n",
    "y_pred_kfold_nb = cross_val_predict(model_nb_2, df_X, df_Y, cv=k_fold)\n",
    "scores = cross_val_score(model_nb_2, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_nb)\n",
    "conf_matrix_nb_kfold = confusion_matrix(df_Y, y_pred_kfold_nb)\n",
    "print(\"Confusion Matrix for Dt using k-fold (leave one out)\")\n",
    "print(conf_matrix_nb_kfold)\n",
    "\n",
    "\n",
    "accuracy_nb_2 = ((conf_matrix_nb_kfold[0,0] + conf_matrix_nb_kfold[1,1])/(conf_matrix_nb_kfold[0,0] +conf_matrix_nb_kfold[0,1]+conf_matrix_nb_kfold[1,0]+conf_matrix_nb_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_nb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_nb_list = []\n",
    "k_nb_specificity = []\n",
    "k_nb_sensitivity = []\n",
    "k_nb_precision = []\n",
    "k_nb_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        \n",
    "        model_nb_new = GaussianNB()\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_nb_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_nb_kfold[0][0]\n",
    "        FP = conf_matrix_nb_kfold[0][1]\n",
    "        FN = conf_matrix_nb_kfold[1][0]\n",
    "        TP = conf_matrix_nb_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_nb_list.append(row)\n",
    "    k_nb_specificity.append(row_specificity)\n",
    "    k_nb_sensitivity.append(row_sensitivity)\n",
    "    k_nb_precision.append(row_precision)\n",
    "    k_nb_f1.append(row_f1)\n",
    "    \n",
    "k_nb_list = pd.DataFrame(k_nb_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_nb_list)    \n",
    "\n",
    "k_nb_specificity = pd.DataFrame(k_nb_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_nb_specificity)\n",
    "\n",
    "k_nb_sensitivity = pd.DataFrame(k_nb_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_nb_sensitivity)\n",
    "\n",
    "k_nb_precision = pd.DataFrame(k_nb_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_nb_precision)\n",
    "\n",
    "k_nb_f1 = pd.DataFrame(k_nb_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Naive Bayes Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_nb_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Logistic Regression Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "\n",
    "\n",
    "model_lr = LogisticRegression(random_state=0)\n",
    "model_lr = model_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "accuracy_lr = ((conf_matrix_lr[0,0] + conf_matrix_lr[1,1])/(conf_matrix_lr[0,0] +conf_matrix_lr[0,1]+conf_matrix_lr[1,0]+conf_matrix_lr[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_lr)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_lr))\n",
    "\n",
    "print(y_pred_lr)\n",
    "\n",
    "print(conf_matrix_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "#Create new LR object\n",
    "model_lr2 = LogisticRegression()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(model_lr2, hyperparameters, cv=cv)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (LR)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(C= 10, penalty='l2',solver= 'newton-cg')\n",
    "model_lr = model_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_lr_2 = confusion_matrix(y_test, y_pred_lr_2)\n",
    "\n",
    "accuracy_lr_2 = ((conf_matrix_lr_2[0,0] + conf_matrix_lr_2[1,1])/(conf_matrix_lr_2[0,0] +conf_matrix_lr_2[0,1]+conf_matrix_lr_2[1,0]+conf_matrix_lr_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_lr_2)\n",
    "print(accuracy_lr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (LR) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_lr_list = []\n",
    "k_lr_specificity = []\n",
    "k_lr_sensitivity = []\n",
    "k_lr_precision = []\n",
    "k_lr_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        model_nb_new = LogisticRegression(C= 10, penalty='l2',solver= 'newton-cg')\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_lr_list.append(row)\n",
    "    k_lr_specificity.append(row_specificity)\n",
    "    k_lr_sensitivity.append(row_sensitivity)\n",
    "    k_lr_precision.append(row_precision)\n",
    "    k_lr_f1.append(row_f1)\n",
    "    \n",
    "k_lr_list = pd.DataFrame(k_lr_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_lr_list)    \n",
    "\n",
    "k_lr_specificity = pd.DataFrame(k_lr_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_lr_specificity)\n",
    "\n",
    "k_lr_sensitivity = pd.DataFrame(k_lr_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_lr_sensitivity)\n",
    "\n",
    "k_lr_precision = pd.DataFrame(k_lr_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_lr_precision)\n",
    "\n",
    "k_lr_f1 = pd.DataFrame(k_lr_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Logistic Regression Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Gradient Boosting Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "\n",
    "model_gb = GradientBoostingClassifier(random_state=0)\n",
    "model_gb = model_gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = model_gb.predict(X_test)\n",
    "\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "accuracy_gb = ((conf_matrix_gb[0,0] + conf_matrix_gb[1,1])/(conf_matrix_gb[0,0] +conf_matrix_gb[0,1]+conf_matrix_gb[1,0]+conf_matrix_gb[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_gb)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_gb))\n",
    "\n",
    "print(y_pred_gb)\n",
    "\n",
    "print(conf_matrix_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200] #[5,50,250,500]\n",
    "max_depth = [1,3,5,7,9]\n",
    "learning_rate = [1, 0.5, 0.25, 0.1, 0.05, 0.01] #[0.01,0.1,1,10,100] \n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(n_estimators=n_estimators,max_depth=max_depth,learning_rate=learning_rate)\n",
    "\n",
    "#Create new LR object\n",
    "model_gb2 = GradientBoostingClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(model_gb2, hyperparameters, cv=10)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (GB)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_gb = GradientBoostingClassifier(learning_rate= 0.5, max_depth=1,n_estimators=3)\n",
    "model_gb = model_gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb_2 = model_gb.predict(X_test)\n",
    "\n",
    "conf_matrix_gb_2 = confusion_matrix(y_test, y_pred_gb_2)\n",
    "\n",
    "accuracy_gb_2 = ((conf_matrix_gb_2[0,0] + conf_matrix_gb_2[1,1])/(conf_matrix_gb_2[0,0] +conf_matrix_gb_2[0,1]+conf_matrix_gb_2[1,0]+conf_matrix_gb_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_gb_2)\n",
    "print(accuracy_gb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (GB) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_gb_list = []\n",
    "k_gb_specificity = []\n",
    "k_gb_sensitivity = []\n",
    "k_gb_precision = []\n",
    "k_gb_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling        \n",
    "        model_nb_new = GradientBoostingClassifier(learning_rate= 0.5, max_depth=1,n_estimators=3)\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_gb_list.append(row)\n",
    "    k_gb_specificity.append(row_specificity)\n",
    "    k_gb_sensitivity.append(row_sensitivity)\n",
    "    k_gb_precision.append(row_precision)\n",
    "    k_gb_f1.append(row_f1)\n",
    "    \n",
    "k_gb_list = pd.DataFrame(k_gb_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_gb_list)    \n",
    "\n",
    "k_gb_specificity = pd.DataFrame(k_gb_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_gb_specificity)\n",
    "\n",
    "k_gb_sensitivity = pd.DataFrame(k_gb_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_gb_sensitivity)\n",
    "\n",
    "k_gb_precision = pd.DataFrame(k_gb_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_gb_precision)\n",
    "\n",
    "k_gb_f1 = pd.DataFrame(k_gb_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_gb_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Random Forest Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "\n",
    "model_gb = RandomForestClassifier()\n",
    "model_gb = model_gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = model_gb.predict(X_test)\n",
    "\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "accuracy_gb = ((conf_matrix_gb[0,0] + conf_matrix_gb[1,1])/(conf_matrix_gb[0,0] +conf_matrix_gb[0,1]+conf_matrix_gb[1,0]+conf_matrix_gb[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_gb)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_gb))\n",
    "\n",
    "print(y_pred_gb)\n",
    "\n",
    "print(conf_matrix_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200] #[5,50,250,500]\n",
    "max_depth = [1,3,5,7,9]\n",
    "max_features = ['auto', 'sqrt', 'log2'] #[0.01,0.1,1,10,100] \n",
    "min_samples_split = [2,5,10]\n",
    "min_samples_leaf = [1,2,5,10,15]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(n_estimators=n_estimators,max_depth=max_depth,max_features=max_features, min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf )\n",
    "\n",
    "#Create new LR object\n",
    "model_gb2 = RandomForestClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(model_gb2, hyperparameters, cv=10)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (Random Forest)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_gb = RandomForestClassifier(n_estimators=16,max_depth=3,max_features='auto', min_samples_split=10,min_samples_leaf=5)\n",
    "model_gb = model_gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb_2 = model_gb.predict(X_test)\n",
    "\n",
    "conf_matrix_gb_2 = confusion_matrix(y_test, y_pred_gb_2)\n",
    "\n",
    "accuracy_gb_2 = ((conf_matrix_gb_2[0,0] + conf_matrix_gb_2[1,1])/(conf_matrix_gb_2[0,0] +conf_matrix_gb_2[0,1]+conf_matrix_gb_2[1,0]+conf_matrix_gb_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_gb_2)\n",
    "print(accuracy_gb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (RF) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "print(kfold)\n",
    "\n",
    "k_rf_list = []\n",
    "k_rf_specificity = []\n",
    "k_rf_sensitivity = []\n",
    "k_rf_precision = []\n",
    "k_rf_f1 = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row_specificity = []\n",
    "    row_sensitivity = []\n",
    "    row_precision = []\n",
    "    row_f1 = []\n",
    "    \n",
    "    row.append(i)\n",
    "    row_specificity.append(i)\n",
    "    row_sensitivity.append(i)\n",
    "    row_precision.append(i)\n",
    "    row_f1.append(i)\n",
    "    \n",
    "    total = 0\n",
    "    total_specificity = 0\n",
    "    total_sensitivity = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        print('\\ntrain: %s, test: %s' % (train.size, test.size))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling              \n",
    "        model_nb_new = RandomForestClassifier(n_estimators=16,max_depth=3,max_features='auto', min_samples_split=10,min_samples_leaf=5)\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "\n",
    "        #accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "        TN = conf_matrix_knn_kfold[0][0]\n",
    "        FP = conf_matrix_knn_kfold[0][1]\n",
    "        FN = conf_matrix_knn_kfold[1][0]\n",
    "        TP = conf_matrix_knn_kfold[1][1]\n",
    "        \n",
    "    \n",
    "        accuracy_knn_kfold = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "        sensitivity_knn_kfold = (TP/(TP+FN)) * 100 #recall\n",
    "        specificity_knn_kfold = (TN/(TN + FP)) * 100\n",
    "        precision_knn_kfold = (TP/(TP+FP)) *100\n",
    "        f1_knn_kfold = 2 *((sensitivity_knn_kfold * precision_knn_kfold)/(sensitivity_knn_kfold + precision_knn_kfold))\n",
    "\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        row_specificity.append(specificity_knn_kfold)\n",
    "        row_sensitivity.append(sensitivity_knn_kfold)\n",
    "        row_precision.append(precision_knn_kfold)\n",
    "        row_f1.append(f1_knn_kfold)\n",
    "        \n",
    "        total += accuracy_knn_kfold\n",
    "        total_specificity += specificity_knn_kfold\n",
    "        total_sensitivity += sensitivity_knn_kfold\n",
    "        total_precision += precision_knn_kfold\n",
    "        total_f1 += f1_knn_kfold\n",
    "\n",
    "    average = row.append(total/parts)   \n",
    "    row_specificity.append(total_specificity/parts)\n",
    "    row_sensitivity.append(total_sensitivity/parts)\n",
    "    row_precision.append(total_precision/parts)\n",
    "    row_f1.append(total_f1/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_rf_list.append(row)\n",
    "    k_rf_specificity.append(row_specificity)\n",
    "    k_rf_sensitivity.append(row_sensitivity)\n",
    "    k_rf_precision.append(row_precision)\n",
    "    k_rf_f1.append(row_f1)\n",
    "    \n",
    "k_rf_list = pd.DataFrame(k_rf_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset Classification Accuracy\")\n",
    "print(k_rf_list)    \n",
    "\n",
    "k_rf_specificity = pd.DataFrame(k_rf_specificity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean specificity'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Specificity\")\n",
    "print(k_rf_specificity)\n",
    "\n",
    "k_rf_sensitivity = pd.DataFrame(k_rf_sensitivity, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean sensitivity/recall'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Sensitivity/Recall\")\n",
    "print(k_rf_sensitivity)\n",
    "\n",
    "k_rf_precision = pd.DataFrame(k_rf_precision, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean precision'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - Precision\")\n",
    "print(k_rf_precision)\n",
    "\n",
    "k_rf_f1 = pd.DataFrame(k_rf_f1, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean f1 score'])\n",
    "print(\"Gradient Boosting Kfold Evaluation for MDVR-KCL Dataset - F1 score\")\n",
    "print(k_rf_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = []\n",
    "\n",
    "##################classifictaion accuracy\n",
    "knn_mean_accuracy = k_list[\"mean accuracy\"].mean()\n",
    "dt_mean_accuracy = k_dt_list[\"mean accuracy\"].mean()\n",
    "svm_mean_accuracy = k_svm_list[\"mean accuracy\"].mean()\n",
    "nb_mean_accuracy = k_nb_list[\"mean accuracy\"].mean()\n",
    "lr_mean_accuracy = k_lr_list[\"mean accuracy\"].mean()\n",
    "gb_mean_accuracy = k_gb_list[\"mean accuracy\"].mean()\n",
    "rf_mean_accuracy = k_rf_list[\"mean accuracy\"].mean()\n",
    "\n",
    "print(knn_mean_accuracy)\n",
    "print(dt_mean_accuracy)\n",
    "print(svm_mean_accuracy)\n",
    "print(nb_mean_accuracy)\n",
    "print(lr_mean_accuracy)\n",
    "print(gb_mean_accuracy)\n",
    "print(rf_mean_accuracy)\n",
    "rf_mean_accuracy\n",
    "\n",
    "classification_accuracy = ['Classification Accuracy',knn_mean_accuracy,dt_mean_accuracy,svm_mean_accuracy,nb_mean_accuracy,lr_mean_accuracy,gb_mean_accuracy,rf_mean_accuracy]\n",
    "df_comparison.append(classification_accuracy)\n",
    "\n",
    "#########################specificity\n",
    "knn_mean_specificity = k_specificity[\"mean specificity\"].mean()\n",
    "dt_mean_specificity = k_dt_specificity[\"mean specificity\"].mean()\n",
    "svm_mean_specificity = k_svm_specificity[\"mean specificity\"].mean()\n",
    "nb_mean_specificity = k_nb_specificity[\"mean specificity\"].mean()\n",
    "lr_mean_specificity = k_lr_specificity[\"mean specificity\"].mean()\n",
    "gb_mean_specificity = k_gb_specificity[\"mean specificity\"].mean()\n",
    "rf_mean_specificity = k_rf_specificity[\"mean specificity\"].mean()\n",
    "\n",
    "specificity = ['Specificity',knn_mean_specificity,dt_mean_specificity,svm_mean_specificity,nb_mean_specificity,lr_mean_specificity,gb_mean_specificity,rf_mean_specificity]\n",
    "df_comparison.append(specificity)\n",
    "\n",
    "#########################sensitivity/recall\n",
    "knn_mean_sensitivity = k_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "dt_mean_sensitivity = k_dt_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "svm_mean_sensitivity = k_svm_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "nb_mean_sensitivity = k_nb_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "lr_mean_sensitivity = k_lr_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "gb_mean_sensitivity = k_gb_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "rf_mean_sensitivity = k_rf_sensitivity[\"mean sensitivity/recall\"].mean()\n",
    "\n",
    "sensitivity = ['Sensitivity',knn_mean_sensitivity,dt_mean_sensitivity,svm_mean_sensitivity,nb_mean_sensitivity,lr_mean_sensitivity,gb_mean_sensitivity,rf_mean_sensitivity]\n",
    "df_comparison.append(sensitivity)\n",
    "\n",
    "#########################precision\n",
    "knn_mean_precision = k_precision[\"mean precision\"].mean()\n",
    "dt_mean_precision = k_dt_precision[\"mean precision\"].mean()\n",
    "svm_mean_precision = k_svm_precision[\"mean precision\"].mean()\n",
    "nb_mean_precision = k_nb_precision[\"mean precision\"].mean()\n",
    "lr_mean_precision = k_lr_precision[\"mean precision\"].mean()\n",
    "gb_mean_precision = k_gb_precision[\"mean precision\"].mean()\n",
    "rf_mean_precision = k_rf_precision[\"mean precision\"].mean()\n",
    "\n",
    "precision = ['Precision',knn_mean_precision,dt_mean_precision,svm_mean_precision,nb_mean_precision,lr_mean_precision,gb_mean_precision,rf_mean_precision]\n",
    "df_comparison.append(precision)\n",
    "\n",
    "#########################F1 score\n",
    "knn_mean_f1 = k_f1[\"mean f1 score\"].mean()/100\n",
    "dt_mean_f1 = k_dt_f1[\"mean f1 score\"].mean()/100\n",
    "svm_mean_f1 = k_svm_f1[\"mean f1 score\"].mean()/100\n",
    "nb_mean_f1 = k_nb_f1[\"mean f1 score\"].mean()/100\n",
    "lr_mean_f1 = k_lr_f1[\"mean f1 score\"].mean()/100\n",
    "gb_mean_f1 = k_gb_f1[\"mean f1 score\"].mean()/100\n",
    "rf_mean_f1 = k_rf_f1[\"mean f1 score\"].mean()/100\n",
    "\n",
    "f1 = ['f1 score',knn_mean_f1,dt_mean_f1,svm_mean_f1,nb_mean_f1,lr_mean_f1,gb_mean_f1,rf_mean_f1]\n",
    "df_comparison.append(f1)\n",
    "\n",
    "df_comparison = pd.DataFrame(df_comparison, columns=[\"Performance Metrics\", \"KNN\", \"Decision Trees\",\"SVM\", \"Naive Bayes\", \"Logistic Regression\", \"GB\", \"Random Forest\"])\n",
    "df_comparison = df_comparison.set_index('Performance Metrics')\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df_comparison.loc[ ['Classification Accuracy' , 'Specificity Accuracy'] , : ]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison.plot(kind=\"bar\", figsize=(15, 10))\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel(\"Average performance values\")\n",
    "plt.title(\"Average values of each performance metric using MDVR-KCL Acoustic Features only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison[['KNN','Decision Trees']].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
